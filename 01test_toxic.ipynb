{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模块加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import multiply\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 目录定义查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "path = '../nocode/'\n",
    "inp = 'input/testproject/'\n",
    "oup = 'output/testproject/'\n",
    "logp = 'logs/testproject/'\n",
    "modp = 'models/testproject/'\n",
    "\n",
    "# print(check_output([\"ls\", path + inp]).decode(\"utf8\"))\n",
    "\n",
    "EMBEDDING_FILE = path + 'wordvector/wiki.zh.vec'\n",
    "EMBEDDING_FILE = path + 'wordvector/crawl-300d-2M.vec'\n",
    "\n",
    "# 训练数据总集\n",
    "data_all_file = path + inp + 'sematic_label_train.csv'\n",
    "\n",
    "# 训练数据集\n",
    "train_file = path + inp + 'train.csv'\n",
    "# 测试数据集\n",
    "test_file = path + inp + 'test_alll.csv'\n",
    "# test_file = path + inp + 'test.csv'\n",
    "\n",
    "TXT_DATA_FILE = path + inp + 'sematic_train.txt'\n",
    "XLSX_DATA_FILE = path + inp + 'sematic_train.xlsx'\n",
    "CSV_DATA_FILE = path + inp + 'sematic_train.csv'\n",
    "\n",
    "# 结果文件\n",
    "res_file = path + inp + 'baseline.csv'\n",
    "# 模型文件\n",
    "model_path = path + modp + 'model_t.h5'\n",
    "out_path = path + oup + 'baseline.csv'\n",
    "tensor_path = path + logp + 'baseline.csv'\n",
    "\n",
    "# model_path = './model/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据集 训练转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate to 1 hot\n",
    "# 去非nan\n",
    "data_all = pd.read_csv(data_all_file)\n",
    "data_all = data_all[~(data_all[\"评论内容\"].isnull())]\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "\n",
    "# 变成one hot\n",
    "data_all['postive']=0\n",
    "data_all['neutral']=0\n",
    "data_all['negative']=0\n",
    "data_all.loc[data_all['分类'] == 1, 'postive'] = 1\n",
    "data_all.loc[data_all['分类'] == 0, 'neutral'] = 1\n",
    "data_all.loc[data_all['分类'] == -1, 'negative'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218296, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.686 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# max_features = 999999\n",
    "max_features = 200000\n",
    "maxlen = 300\n",
    "memn = 100\n",
    "dropn = 0.5\n",
    "\n",
    "# train_file = path + inp + 'train.csv'\n",
    "train = pd.read_csv(train_file)\n",
    "print(train.shape)\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "test = pd.read_csv(test_file)\n",
    "print(test.shape)\n",
    "for i1 in train.index:\n",
    "    train.loc[i1, \"评论内容\"]=\" \".join(jieba.cut(train.loc[i1, \"评论内容\"]))\n",
    "for i1 in test.index:\n",
    "    test.loc[i1, \"评论内容\"]=\" \".join(jieba.cut(test.loc[i1, \"评论内容\"]))\n",
    "    \n",
    "list_sentences_train = train[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "# list_classes = [i1 for i1 in train.columns]\n",
    "list_classes = [\"negative\",\"neutral\",\"postive\"]\n",
    "try:\n",
    "    list_classes.remove(\"评论内容\")\n",
    "    list_classes.remove(\"id\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "y = train[list_classes].values\n",
    "\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(memn,dropn):\n",
    "    embed_size = 300\n",
    "    # 时间步 = maxlen\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    # memory units = 50\n",
    "    x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(memn, activation=\"relu\")(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(3, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          60000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 200)          320800    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 60,341,203\n",
      "Trainable params: 60,341,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early, tensorb] #early\n",
    "\n",
    "model = get_model(memn, dropn)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                评论内容   评论数    点赞数  分类  \\\n",
      "0                                  肖杰   实力派   有 资本 狂   1.0   73.0   1   \n",
      "1                                 鹿晗 鹿晗 ， 一鹿 伴晗 ， 加油  26.0  174.0   1   \n",
      "2    冯 老板 的 这段 舞 真的 精彩 ， 那个 慢动作 太帅 了 ， 看 了 N 遍 了 ， 赞   3.0   36.0   1   \n",
      "3                        感觉 有 黑幕 ， 陈伟霆队 跳 的 更好 ， ， ，   0.0    7.0  -1   \n",
      "4  白羊座 本来 就 很 冲动 一点 的 ， 所以 请 任何人 不要 这样 说 鹿晗 ， 换个 ...   0.0    3.0  -1   \n",
      "\n",
      "   postive  neutral  negative  \n",
      "0        1        0         0  \n",
      "1        1        0         0  \n",
      "2        1        0         0  \n",
      "3        0        0         1  \n",
      "4        0        0         1  \n",
      "                                                评论内容  评论数   点赞数  分类  postive  \\\n",
      "0                    商人会关心科学？如果是，那应该也是如何科学赚钱的方法吧[捂脸]  NaN  23.0   0        0   \n",
      "1     川普:盖兄到我这来当差如何？\\n盖茨:去你妹的，滚！你有我有钱？我捐出去的钱都比你有的钱多！  NaN  19.0   0        0   \n",
      "2                     比尔盖茨干这个确实屈才了，这个也不是盖茨强项，而且耽误干大事  NaN   8.0   0        0   \n",
      "3  特朗普在没当选总统之前，在盖茨眼里都不算什么，财富是一方面，格局是另一方面，现在让盖茨屈尊伺...  NaN  16.0   0        0   \n",
      "4  敢公然拒绝书记，不，是总统，信不信你公司今年评不了驰名品牌，信不信你公司明年315晚会曝光你...  NaN  28.0   0        0   \n",
      "\n",
      "   neutral  negative  \n",
      "0        0         0  \n",
      "1        0         0  \n",
      "2        0         0  \n",
      "3        0         0  \n",
      "4        0         0  \n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "# predict(self, x, batch_size=32, verbose=0)\n",
    "# predict_classes(self, x, batch_size=32, verbose=1)\n",
    "# predict_proba(self, x, batch_size=32, verbose=1)\n",
    "# evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "sample_submission = pd.read_csv(test_file)\n",
    "sample_submission[\"分类\"]=0\n",
    "sample_submission[\"postive\"]=0\n",
    "sample_submission[\"neutral\"]=0\n",
    "sample_submission[\"negative\"]=0\n",
    "print(train.head())\n",
    "print(sample_submission.head())\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission[\"max\"]=sample_submission[list_classes].max(axis=1)\n",
    "\n",
    "for indexs in sample_submission.index:  \n",
    "    for  i2 in list_classes:  \n",
    "        if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "            sample_submission.loc[indexs,\"predict\"]=i2\n",
    "for i1 in list_classes:\n",
    "    sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "sample_submission.to_csv(res_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/737 [==============================] - 2s 2ms/step\n",
      "[0.2558635689692226, 0.9081863470608053]\n",
      "['negative', 'neutral', 'postive']\n",
      "[104474, 0, 113822]\n",
      "[115009.34027261566, 19083.862225225195, 89660.32110563666]\n",
      "[0.3392283906962068, 0.2107909471513053, 0.3128077736362239]\n"
     ]
    }
   ],
   "source": [
    "# 正确率评估\n",
    "score = model.evaluate(X_t, y, batch_size=batch_size)\n",
    "print(score)\n",
    "print(list_classes)\n",
    "res_pd = pd.read_csv(res_file)\n",
    "type_ori_num = [res_pd[(res_pd[\"predict\"] == m)].shape[0] for m in list_classes]\n",
    "print(type_ori_num)\n",
    "type_num = [res_pd[\"pred_\" + m].sum() for m in list_classes]\n",
    "print(type_num)\n",
    "\n",
    "# 3. 能量转化\n",
    "Nall =  219268\n",
    "k = 10\n",
    "Nall *= k\n",
    "Et = 1\n",
    "# Ep = k * (Et - 1 / np.log(resnp / Nall))\n",
    "# Ep = -Et / np.log(resnp / Nall)\n",
    "energy = [-Et / np.log(n / Nall) for n in type_num]\n",
    "print(energy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
