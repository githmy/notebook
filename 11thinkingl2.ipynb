{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Improved LSTM baseline\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "import jieba\n",
    "import glob\n",
    "import json\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [],
   "source": [
    "bpath = os.path.join(\"..\", \"data\")\n",
    "\n",
    "TMP_TRAIN_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_origin.csv\")\n",
    "TMP_LABEL_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_label.csv\")\n",
    "TMP_NOLABEL_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_nolabel.csv\")\n",
    "train = pd.read_csv(TMP_TRAIN_FILE, header=0, encoding=\"utf8\", sep=',')\n",
    "predict_pd = pd.read_csv(TMP_LABEL_FILE, header=0, encoding=\"utf8\", sep=',')\n",
    "nolabel = pd.read_csv(TMP_NOLABEL_FILE, header=0, encoding=\"utf8\", sep=',')\n",
    "\n",
    "jieba_userdicts = glob.glob(os.path.join(bpath, \"jieba\", \"*.txt\"))\n",
    "for jieba_userdict in jieba_userdicts:\n",
    "    jieba.load_userdict(jieba_userdict)\n",
    "    print(\"load dict:\",jieba_userdicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id  level mainReviewPoints reviewPoints  \\\n",
      "0  RJB080B1230021      2       二次根式的乘法和除法   二次根式的乘法和除法   \n",
      "1  RJB080A4321001      2      因式分解-平方差公式法  因式分解-平方差公式法   \n",
      "2          4B4306      1           小练习（1）       小练习（1）   \n",
      "3  RJB070A3310004      2      解一元一次方程-去括号  解一元一次方程-去括号   \n",
      "4      ZKBCS06019      1            分式的计算        分式的计算   \n",
      "\n",
      "                                                text  \n",
      "0  计算 ：   $ a \\ sqrt { \\ frac { 2b } { a } } \\ di...  \n",
      "1                       分解 因 式   $ 1 - 36b ^ { 2 } $  \n",
      "2                                 小 练习 （ 1 ） - 判断 对错  \n",
      "3  $ y = 2 $   是 方程   $ 2 - ( m - y ) = 3y $   的 ...  \n",
      "4  先 化简   ， 再 求值 ： $ \\ frac { x } { x - 4 } + \\ f...  \n",
      "               id mainReviewPoints reviewPoints  \\\n",
      "0  RJB080B1230021       二次根式的乘法和除法   二次根式的乘法和除法   \n",
      "1  RJB080A4321001      因式分解-平方差公式法  因式分解-平方差公式法   \n",
      "2          4B4306           小练习（1）       小练习（1）   \n",
      "3  RJB070A3310004      解一元一次方程-去括号  解一元一次方程-去括号   \n",
      "4      ZKBCS06019            分式的计算        分式的计算   \n",
      "\n",
      "                                                text  level_0  level_1  \\\n",
      "0  计算 ：   $ a \\ sqrt { \\ frac { 2b } { a } } \\ di...        0        0   \n",
      "1                       分解 因 式   $ 1 - 36b ^ { 2 } $        0        0   \n",
      "2                                 小 练习 （ 1 ） - 判断 对错        0        1   \n",
      "3  $ y = 2 $   是 方程   $ 2 - ( m - y ) = 3y $   的 ...        0        0   \n",
      "4  先 化简   ， 再 求值 ： $ \\ frac { x } { x - 4 } + \\ f...        0        1   \n",
      "\n",
      "   level_2  level_4  level_5  level_6  \n",
      "0        1        0        0        0  \n",
      "1        1        0        0        0  \n",
      "2        0        0        0        0  \n",
      "3        1        0        0        0  \n",
      "4        0        0        0        0  \n",
      "(2491, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "list_sentences_train = train[\"text\"].fillna(\"_na_\").values\n",
    "train = pd.get_dummies(train, columns=['level'])\n",
    "\n",
    "# # 训练的标签\n",
    "# yc = np.hstack((ym, yr))\n",
    "list_classes = [i1 for i1 in train.columns if i1.startswith(\"level_\")]\n",
    "yl = train[list_classes].values\n",
    "\n",
    "print(train.head())\n",
    "# print(yc.shape)\n",
    "print(yl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [],
   "source": [
    "# 预测的值\n",
    "# list_sentences_test = predict_pd[\"Description\"].fillna(\"_na_\").values\n",
    "list_sentences_test_label = predict_pd[\"text\"].fillna(\"_na_\").values\n",
    "list_sentences_test_nolabel = nolabel[\"text\"].fillna(\"_na_\").values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test_label = tokenizer.texts_to_sequences(list_sentences_test_label)\n",
    "list_tokenized_test_nolabel = tokenizer.texts_to_sequences(list_sentences_test_nolabel)\n",
    "X_tr = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te_label = pad_sequences(list_tokenized_test_label, maxlen=maxlen)\n",
    "X_te_nolabel = pad_sequences(list_tokenized_test_nolabel, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = os.path.join(bpath, \"wordvector\", \"crawl-300d-2M.vec\")\n",
    "EMBEDDING_FILE = os.path.join(bpath, \"wordvector\", \"wiki.zh.vec\")\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf-8\"))\n",
    "for o in list(embeddings_index.keys()):\n",
    "     if len(embeddings_index[o])!=embed_size:\n",
    "         del embeddings_index[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python365\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)+2)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[0]=np.zeros((embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inp_l = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp_l)\n",
    "x = Bidirectional(LSTM(150, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(len(list_classes), activation=\"softmax\")(x)\n",
    "model_l = Model(inputs=inp_l, outputs=x)\n",
    "model_l.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=1000\n",
    "tensor_path_l = os.path.join(bpath, \"logs\", \"thinking2_l\")\n",
    "model_path_l = os.path.join(bpath, \"model\", \"thinking2_l\",\"rasa_weights_base.best.hdf5\")\n",
    "checkpoint = ModelCheckpoint(model_path_l, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path_l, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "callbacks_list_l = [checkpoint, early, tensorb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2241 samples, validate on 250 samples\n",
      "Epoch 1/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.8226 - acc: 0.6488 - val_loss: 0.6844 - val_acc: 0.7120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68437, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 2/1000\n",
      "2241/2241 [==============================] - 17s 8ms/step - loss: 0.6916 - acc: 0.7171 - val_loss: 0.6571 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68437 to 0.65713, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 3/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.6441 - acc: 0.7372 - val_loss: 0.6015 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65713 to 0.60150, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 4/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.5583 - acc: 0.7925 - val_loss: 0.5880 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60150 to 0.58805, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 5/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.5218 - acc: 0.7983 - val_loss: 0.5782 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58805 to 0.57825, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 6/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.4734 - acc: 0.8233 - val_loss: 0.5999 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57825\n",
      "Epoch 7/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.3893 - acc: 0.8617 - val_loss: 0.5620 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57825 to 0.56198, saving model to ..\\data\\model\\thinking2_l\\rasa_weights_base.best.hdf5\n",
      "Epoch 8/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.3398 - acc: 0.8746 - val_loss: 0.5986 - val_acc: 0.7560\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.56198\n",
      "Epoch 9/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.3004 - acc: 0.9063 - val_loss: 0.6889 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.56198\n",
      "Epoch 10/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.2684 - acc: 0.9099 - val_loss: 0.6635 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56198\n",
      "Epoch 11/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.2263 - acc: 0.9228 - val_loss: 0.6979 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56198\n",
      "Epoch 12/1000\n",
      "2241/2241 [==============================] - 17s 8ms/step - loss: 0.2065 - acc: 0.9340 - val_loss: 0.7385 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.56198\n",
      "Epoch 13/1000\n",
      "2241/2241 [==============================] - 17s 8ms/step - loss: 0.1853 - acc: 0.9402 - val_loss: 0.6988 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.56198\n",
      "Epoch 14/1000\n",
      "2241/2241 [==============================] - 19s 8ms/step - loss: 0.1586 - acc: 0.9456 - val_loss: 0.7367 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56198\n",
      "Epoch 15/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.1461 - acc: 0.9554 - val_loss: 0.7392 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.56198\n",
      "Epoch 16/1000\n",
      "2241/2241 [==============================] - 17s 8ms/step - loss: 0.1252 - acc: 0.9594 - val_loss: 0.7736 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.56198\n",
      "Epoch 17/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.1116 - acc: 0.9634 - val_loss: 0.7511 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.56198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24098cfb128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l.fit(X_tr, yl, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l.load_weights(model_path_l)\n",
    "y_test_l_label = model_l.predict(X_te_label)\n",
    "y_test_l_nolabel = model_l.predict(X_te_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'mainReviewPoints', 'reviewPoints', 'level',\n",
      "       'mainReviewPoints_new'],\n",
      "      dtype='object')\n",
      "Index(['id', 'text', 'mainReviewPoints', 'reviewPoints', 'level',\n",
      "       'mainReviewPoints_new'],\n",
      "      dtype='object')\n",
      "(277, 6)\n",
      "(1029, 6)\n",
      "finish output csv.\n"
     ]
    }
   ],
   "source": [
    "# sample_submission_label = pd.read_csv(TMP_LABEL_FILE, header=0, encoding=\"GBK\", dtype=str,sep=',')\n",
    "sample_submission_label = pd.read_csv(TMP_LABEL_FILE, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "sample_submission_nolabel = pd.read_csv(TMP_NOLABEL_FILE, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "print(sample_submission_label.columns)\n",
    "print(sample_submission_nolabel.columns)\n",
    "\n",
    "sample_submission_label=sample_submission_label[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\", \"mainReviewPoints_new\"]]\n",
    "sample_submission_nolabel=sample_submission_nolabel[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\", \"mainReviewPoints_new\"]]\n",
    "# sample_submission=sample_submission[[\"Description\", \"Level\"]]\n",
    "# 结果转化输出\n",
    "\n",
    "print(y_test_l_label.shape)\n",
    "\n",
    "sample_submission_label[\"level_new\"] = np.argmax(y_test_l_label, axis=1)\n",
    "sample_submission_label=sample_submission_label[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\",\"mainReviewPoints_new\", \"level_new\"]]\n",
    "sample_submission_label.to_csv(TMP_LABEL_FILE, index=False, encoding=\"utf-8\")\n",
    "# sample_submission_label.to_csv(TMP_LABEL_FILE, index=False, encoding=\"GBK\")\n",
    "\n",
    "print(y_test_l_nolabel.shape)\n",
    "\n",
    "\n",
    "# for indexs in sample_submission.index:  \n",
    "#     for  i2 in list_classes:  \n",
    "#         if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "#             sample_submission.loc[indexs,\"predict\"]=i2\n",
    "# for i1 in list_classes:\n",
    "#     sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "sample_submission_nolabel[\"level_new\"] = np.argmax(y_test_l_nolabel, axis=1)\n",
    "sample_submission_nolabel=sample_submission_nolabel[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\",\"mainReviewPoints_new\", \"level_new\"]]\n",
    "sample_submission_nolabel.to_csv(TMP_NOLABEL_FILE, index=False, encoding=\"utf-8\")\n",
    "# sample_submission_nolabel.to_csv(TMP_NOLABEL_FILE, index=False, encoding=\"GBK\")\n",
    "\n",
    "# print(sample_submission[\"mainReviewPoints\"])\n",
    "# print(sample_submission[\"mainReviewPoints\"][0])\n",
    "# print(type(sample_submission[\"mainReviewPoints\"][0]))\n",
    "# print(sample_submission[sample_submission[\"mainReviewPoints\"]!=\"\"])\n",
    "print(\"finish output csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id                                               text  \\\n",
      "0  RJB070B6200011  已知 数据 总数 是   $ 30 $   ， 在 样本 频数 分布 直方图 （ 如下 图 ...   \n",
      "1          4B1206                            整数 的 运算 性质 - 填写数字 和 运算符   \n",
      "2  RJB090B3220015  如图 ，   $ \\ bigodot   O $   的 半径 为   $ 1cm $   ...   \n",
      "3      K050301009       如果   $ | x | + x = 0 $ ， 那么   $ ( \\ quad ) $   \n",
      "4  RJB080B1230020  计算 ：   $ 2 \\ sqrt { x ^ { 2 } y } \\ div   \\ sq...   \n",
      "\n",
      "  mainReviewPoints                reviewPoints level mainReviewPoints_new  \\\n",
      "0            频数直方图                       频数直方图     2                  NaN   \n",
      "1          整数的运算性质                     整数的运算性质     1                  NaN   \n",
      "2       解直角三角形应用举例  三角形的角平分线,解直角三角形,解直角三角形应用举例     2                  NaN   \n",
      "3              绝对值                         绝对值     0                  NaN   \n",
      "4       二次根式的乘法和除法                  二次根式的乘法和除法     2                  NaN   \n",
      "\n",
      "   level_new  \n",
      "0          2  \n",
      "1          1  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "277\n",
      "79.42238267148014%\n",
      "10.830324909747292%\n"
     ]
    }
   ],
   "source": [
    "print(sample_submission_label.head())\n",
    "lenth0 = sample_submission_label.shape[0]\n",
    "print(lenth0)\n",
    "# print(sample_submission_label.loc[sample_submission_label['level']==sample_submission_label[\"level_new\"]].shape)\n",
    "# print(sample_submission_label[sample_submission_label[\"level\"]==sample_submission_label[\"level\"]].shape)\n",
    "# print(sample_submission_label[(sample_submission_label[\"mainReviewPoints\"]==sample_submission_label[\"mainReviewPoints_new\"]) & sample_submission_label[\"mainReviewPoints_new\"].notnull()\n",
    "#                               & sample_submission_label[\"mainReviewPoints_new\"]==\"公式法解一元二次方程\"].shape)\n",
    "lev_c = 0\n",
    "mai_c = 0\n",
    "for i1 in sample_submission_label.index:\n",
    "    if int(sample_submission_label.loc[i1,\"level\"])==int(sample_submission_label.loc[i1,\"level_new\"]):\n",
    "        lev_c += 1\n",
    "    if sample_submission_label.loc[i1,\"mainReviewPoints\"]==sample_submission_label.loc[i1,\"mainReviewPoints_new\"]:\n",
    "        mai_c += 1\n",
    "print(\"{}%\".format(lev_c/lenth0*100))\n",
    "print(\"{}%\".format(mai_c/lenth0*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
