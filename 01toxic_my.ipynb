{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模块加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import multiply\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 目录定义查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "path = '../nocode/'\n",
    "inp = 'input/testproject/'\n",
    "oup = 'output/testproject/'\n",
    "logp = 'logs/testproject/'\n",
    "modp = 'models/testproject/'\n",
    "\n",
    "# print(check_output([\"ls\", path + inp]).decode(\"utf8\"))\n",
    "\n",
    "EMBEDDING_FILE = path + 'wordvector/wiki.zh.vec'\n",
    "EMBEDDING_FILE = path + 'wordvector/crawl-300d-2M.vec'\n",
    "\n",
    "# 训练数据总集\n",
    "data_all_file = path + inp + 'sematic_label_train.csv'\n",
    "\n",
    "# 训练数据集\n",
    "train_file = path + inp + 'train.csv'\n",
    "# 测试数据集\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "test_file = path + inp + 'test.csv'\n",
    "\n",
    "TXT_DATA_FILE = path + inp + 'sematic_train.txt'\n",
    "XLSX_DATA_FILE = path + inp + 'sematic_train.xlsx'\n",
    "CSV_DATA_FILE = path + inp + 'sematic_train.csv'\n",
    "\n",
    "# 结果文件\n",
    "res_file = path + inp + 'baseline.csv'\n",
    "# 模型文件\n",
    "model_path = path + modp + 'model_t.h5'\n",
    "out_path = path + oup + 'baseline.csv'\n",
    "tensor_path = path + logp + 'baseline.csv'\n",
    "\n",
    "# model_path = './model/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据集 训练转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate to 1 hot\n",
    "# 去非nan\n",
    "data_all = pd.read_csv(data_all_file)\n",
    "data_all = data_all[~(data_all[\"评论内容\"].isnull())]\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "\n",
    "# 变成one hot\n",
    "data_all['postive']=0\n",
    "data_all['neutral']=0\n",
    "data_all['negative']=0\n",
    "data_all.loc[data_all['分类'] == 1, 'postive'] = 1\n",
    "data_all.loc[data_all['分类'] == 0, 'neutral'] = 1\n",
    "data_all.loc[data_all['分类'] == -1, 'negative'] = 1\n",
    "\n",
    "# # 拆分测试集,训练时跳过  *******************************\n",
    "# train_all = data_all.sample(frac=1).reset_index(drop=True)\n",
    "# lenth_train = train_all.shape[0]\n",
    "# spint = int(1*lenth_train)\n",
    "# train = train_all.loc[0:spint,:]\n",
    "# test = train_all.loc[spint:,:]\n",
    "# train.to_csv(train_file, index=False)\n",
    "# test.to_csv(test_file, index=False)\n",
    "# # 拆分测试集,训练时跳过  *******************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737, 7)\n",
      "(184, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# max_features = 999999\n",
    "max_features = 200000\n",
    "maxlen = 300\n",
    "memn = 100\n",
    "dropn = 0.5\n",
    "\n",
    "# train_file = path + inp + 'train.csv'\n",
    "train = pd.read_csv(train_file)\n",
    "print(train.shape)\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "test = pd.read_csv(test_file)\n",
    "print(test.shape)\n",
    "for i1 in train.index:\n",
    "    train.loc[i1, \"评论内容\"]=\" \".join(jieba.cut(train.loc[i1, \"评论内容\"]))\n",
    "for i1 in test.index:\n",
    "    test.loc[i1, \"评论内容\"]=\" \".join(jieba.cut(test.loc[i1, \"评论内容\"]))\n",
    "    \n",
    "list_sentences_train = train[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "# list_classes = [i1 for i1 in train.columns]\n",
    "list_classes = [\"negative\",\"neutral\",\"postive\"]\n",
    "try:\n",
    "    list_classes.remove(\"评论内容\")\n",
    "    list_classes.remove(\"id\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "y = train[list_classes].values\n",
    "\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way attention\n",
    "# def attention_3d_block(inputs):\n",
    "#     #input_dim = int(inputs.shape[2])\n",
    "# #     a = Permute((2, 1))(inputs)\n",
    "#     x = Reshape((maxlen * ,), name='reshape_1')(x)\n",
    "#     a_probs = Dense(maxlen, activation='softmax')(inputs)\n",
    "# #     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "#     return output_attention_mul\n",
    "\n",
    "# lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm')(drop1)\n",
    "# attention_mul = attention_3d_block(lstm_out)\n",
    "# attention_flatten = Flatten()(attention_mul)\n",
    "# drop2 = Dropout(0.3)(attention_flatten)\n",
    "\n",
    "def get_model(memn,dropn):\n",
    "    embed_size = 105\n",
    "    embed_size = 300\n",
    "    # 时间步 = maxlen\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    # memory units = 50\n",
    "    x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "# #     x = GlobalMaxPool1D()(x)\n",
    "# #     x = Reshape((maxlen * memn,), name='reshape_1')(x)\n",
    "#     a_probs = Dense(maxlen, activation='softmax')(x)\n",
    "# #     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     x = multiply([x, a_probs], name='attention_mul')\n",
    "\n",
    "# #     x = attention_3d_block(x)\n",
    "#     x = Dropout(dropn)(x)\n",
    "#     x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(memn, activation=\"relu\")(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(3, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          60000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 200)          320800    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 60,341,203\n",
      "Trainable params: 60,341,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early, tensorb] #early\n",
    "\n",
    "# model = get_model(200, 0.2)\n",
    "model = get_model(memn, dropn)\n",
    "print(model.summary())\n",
    "# mlist = list(range(1,7))\n",
    "# droplist = list(range(1,7))\n",
    "# for memn in mlist:\n",
    "#     for dropn in droplist:\n",
    "#         model = get_model(memn*50, dropn*0.1)\n",
    "#         start = time.time()\n",
    "#         model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "#         end = time.time()\n",
    "#         print(end-start)\n",
    "#         print(memn*50, dropn*0.1)\n",
    "#         print(\"*\".center(100,\"#\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 589 samples, validate on 148 samples\n",
      "Epoch 1/100\n",
      "589/589 [==============================] - 18s 30ms/step - loss: 0.6049 - acc: 0.6333 - val_loss: 0.6253 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62535, saving model to ../nocode/models/testproject/model_t.h5\n",
      "Epoch 2/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.5455 - acc: 0.6814 - val_loss: 0.5891 - val_acc: 0.7027\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62535 to 0.58914, saving model to ../nocode/models/testproject/model_t.h5\n",
      "Epoch 3/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.5309 - acc: 0.7063 - val_loss: 0.5704 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58914 to 0.57036, saving model to ../nocode/models/testproject/model_t.h5\n",
      "Epoch 4/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.4262 - acc: 0.8387 - val_loss: 0.4995 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57036 to 0.49947, saving model to ../nocode/models/testproject/model_t.h5\n",
      "Epoch 5/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.2866 - acc: 0.9213 - val_loss: 0.4740 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49947 to 0.47402, saving model to ../nocode/models/testproject/model_t.h5\n",
      "Epoch 6/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.2121 - acc: 0.9360 - val_loss: 0.5451 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47402\n",
      "Epoch 7/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.1735 - acc: 0.9440 - val_loss: 0.5111 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.47402\n",
      "Epoch 8/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.1578 - acc: 0.9496 - val_loss: 0.5357 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.47402\n",
      "Epoch 9/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0932 - acc: 0.9677 - val_loss: 0.5914 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.47402\n",
      "Epoch 10/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0701 - acc: 0.9694 - val_loss: 0.6234 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.47402\n",
      "Epoch 11/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0539 - acc: 0.9757 - val_loss: 0.6944 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.47402\n",
      "Epoch 12/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0361 - acc: 0.9904 - val_loss: 0.8698 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.47402\n",
      "Epoch 13/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0222 - acc: 0.9960 - val_loss: 1.0547 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.47402\n",
      "Epoch 14/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0223 - acc: 0.9949 - val_loss: 1.2106 - val_acc: 0.7658\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47402\n",
      "Epoch 15/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0185 - acc: 0.9977 - val_loss: 1.1726 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47402\n",
      "Epoch 16/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0135 - acc: 0.9989 - val_loss: 1.1688 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47402\n",
      "Epoch 17/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0076 - acc: 0.9977 - val_loss: 1.1968 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47402\n",
      "Epoch 18/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0072 - acc: 0.9989 - val_loss: 1.1248 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47402\n",
      "Epoch 19/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 1.2070 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47402\n",
      "Epoch 20/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 1.2025 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47402\n",
      "Epoch 21/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.2678 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47402\n",
      "Epoch 22/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.3617 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47402\n",
      "Epoch 23/100\n",
      "589/589 [==============================] - 16s 28ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.3092 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47402\n",
      "Epoch 24/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.3473 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47402\n",
      "Epoch 25/100\n",
      "589/589 [==============================] - 17s 28ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.4116 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47402\n",
      "479.4609456062317\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                评论内容   评论数    点赞数  分类  \\\n",
      "0                                  肖杰   实力派   有 资本 狂   1.0   73.0   1   \n",
      "1                                 鹿晗 鹿晗 ， 一鹿 伴晗 ， 加油  26.0  174.0   1   \n",
      "2    冯 老板 的 这段 舞 真的 精彩 ， 那个 慢动作 太帅 了 ， 看 了 N 遍 了 ， 赞   3.0   36.0   1   \n",
      "3                        感觉 有 黑幕 ， 陈伟霆队 跳 的 更好 ， ， ，   0.0    7.0  -1   \n",
      "4  白羊座 本来 就 很 冲动 一点 的 ， 所以 请 任何人 不要 这样 说 鹿晗 ， 换个 ...   0.0    3.0  -1   \n",
      "\n",
      "   postive  neutral  negative  \n",
      "0        1        0         0  \n",
      "1        1        0         0  \n",
      "2        1        0         0  \n",
      "3        0        0         1  \n",
      "4        0        0         1  \n",
      "                                                评论内容   评论数    点赞数  分类  \\\n",
      "0                                        鹿晗第一，鹿晗舞蹈精灵  14.0  194.0   0   \n",
      "1  卜凡好帅好可爱，学的好认真，虽然偶像练习生最终没有选你出道，那不是你的问题，只是名额有限，不...   0.0   31.0   0   \n",
      "2  钢王队错了？最少，大饼错了。很多人说不出错在什麽地方，让我来解释一下。「蝴蝶效应」不但是指一...  71.0   35.0   0   \n",
      "3                                       什么破节目组，水都不给喝   1.0   15.0   0   \n",
      "4        你就是最好的，加油(ง •̀_•́)ง，不要在乎别人说什么我们因为有芦苇跟你一起奋战。  47.0  228.0   0   \n",
      "\n",
      "   postive  neutral  negative  \n",
      "0        0        0         0  \n",
      "1        0        0         0  \n",
      "2        0        0         0  \n",
      "3        0        0         0  \n",
      "4        0        0         0  \n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "# predict(self, x, batch_size=32, verbose=0)\n",
    "# predict_classes(self, x, batch_size=32, verbose=1)\n",
    "# predict_proba(self, x, batch_size=32, verbose=1)\n",
    "# evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "sample_submission = pd.read_csv(test_file)\n",
    "sample_submission[\"分类\"]=0\n",
    "sample_submission[\"postive\"]=0\n",
    "sample_submission[\"neutral\"]=0\n",
    "sample_submission[\"negative\"]=0\n",
    "print(train.head())\n",
    "print(sample_submission.head())\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission[\"max\"]=sample_submission[list_classes].max(axis=1)\n",
    "\n",
    "for indexs in sample_submission.index:  \n",
    "    for  i2 in list_classes:  \n",
    "        if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "            sample_submission.loc[indexs,\"predict\"]=i2\n",
    "for i1 in list_classes:\n",
    "    sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "sample_submission.to_csv(res_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/737 [==============================] - 2s 2ms/step\n",
      "[0.2558635689692226, 0.9081863470608053]\n",
      "negative, sum_num: 88, right_num: 80, accuracy: 0.909\n",
      "neutral, sum_num: 13, right_num: 0, accuracy: 0.000\n",
      "postive, sum_num: 83, right_num: 70, accuracy: 0.843\n",
      "total data, total_num: 184, total_right: 150, accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "# 正确率评估\n",
    "score = model.evaluate(X_t, y, batch_size=batch_size)\n",
    "print(score)\n",
    "test_pd = pd.read_csv(test_file)\n",
    "res_pd = pd.read_csv(res_file)\n",
    "total_pd=pd.concat([res_pd,test_pd], join='outer', axis=1)\n",
    "total_right=0\n",
    "total_num=0\n",
    "for i1 in list_classes:\n",
    "    tmp_obj=total_pd[total_pd[i1] == 1]\n",
    "    sum_num=tmp_obj.shape[0]\n",
    "    right_num=tmp_obj[tmp_obj[\"predict\"] == i1].shape[0]\n",
    "    total_right += right_num\n",
    "    total_num += sum_num\n",
    "    try:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, accuracy: %.3f\" % (i1,sum_num,right_num,right_num/sum_num))\n",
    "    except Exception as e:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, error: %s\" % (i1,sum_num,right_num,str(e)))\n",
    "        \n",
    "print(\"total data, total_num: %d, total_right: %d, accuracy: %.3f\" % (total_num,total_right,total_right/total_num))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
