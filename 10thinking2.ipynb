{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模块加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import multiply\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpo_path = os.path.join(\"..\", \"data\", \"thinking2\", \"predict_obj.csv\")\n",
    "predict_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep='\\t')\n",
    "tmpo_path = os.path.join(\"..\", \"data\", \"thinking2\", \"question_obj.csv\")\n",
    "train_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "tmpo_path = os.path.join(\"..\", \"data\", \"thinking2\", \"review_obj.csv\")\n",
    "dict_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "\n",
    "EMBEDDING_FILE = os.path.join(\"..\", \"data\", \"wordvector\", \"wiki.zh.vec\")\n",
    "EMBEDDING_FILE = os.path.join(\"..\", \"data\", \"wordvector\", \"crawl-300d-2M.vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  level mainReviewPoints reviewPoints                 text\n",
      "0     1      ['A010101']  ['A010101']                  说一说\n",
      "1     1      ['A010101']  ['A010101']                  说一说\n",
      "2     1      ['A010101']  ['A010101']                  说一说\n",
      "3     1      ['A010102']  ['A010102']  看一看，然后将篮子按照不同属性进行分类\n",
      "4     1      ['A010102']  ['A010102']  看一看，然后将车子按照不同属性进行分类\n",
      "                                         Description MainReviewPoints  \\\n",
      "0                                  如图，右边正方体的展开图为(　　)              NaN   \n",
      "1                  如图，这是一个正方体的展开图，则号码2代表的面所相对的面的号码是．              NaN   \n",
      "2  将一张长与宽的比为2︰1的长方形纸片按如图(1)(2)所示的方式对折，然后沿图(3)中的虚线...              NaN   \n",
      "3  图中的甲、乙是否是几何体的平面展开图，先想一想，再折一折，如果是，请说出折叠后的几何体名称、...              NaN   \n",
      "4  如图，有正方体纸盒，在它的三个侧面分别画有三角形、正方形和圆，现用一把剪刀沿着它的棱剪开成一...              NaN   \n",
      "\n",
      "  ReviewPoints Level  \n",
      "0          NaN     1  \n",
      "1          NaN     2  \n",
      "2          NaN     1  \n",
      "3          NaN     2  \n",
      "4          NaN     1  \n",
      "['A010101', 'A010102', 'A010103', 'A010104', 'A010105', 'A010106', 'A010201', 'A010202', 'A010203', 'A010204', 'A010205', 'A010206', 'A010207', 'A010208', 'A010209', 'A010210', 'A010301', 'A010302', 'A010303', 'A010304', 'A010305', 'A010306', 'A010307', 'A010308', 'A010401', 'A010501', 'A010502', 'A010503', 'A010504', 'A010505', 'A010506', 'A010507', 'A010508', 'A010601', 'A010602', 'A010603', 'A010701', 'A010702', 'A010703', 'A010704', 'A010705', 'A010706', 'A010801', 'A010802', 'A010901', 'A010902', 'A010903', 'A010904', 'A010905', 'A010906', 'A011001', 'A011002', 'A011003', 'A011004', 'A011005', 'A011101', 'A011102', 'A011103', 'A011104', 'A011105', 'A020101', 'A020102', 'A020103', 'A020104', 'A020201', 'A020202', 'A020203', 'A020204', 'A020205', 'A020206', 'A020207', 'A020208', 'A020209', 'A020210', 'A020211', 'A020212', 'A020213', 'A020214', 'A020301', 'A020302', 'A020401', 'A020402', 'A020403', 'A020404', 'A020405', 'A020406', 'A020407', 'A020408', 'A020409', 'A020410', 'A020411', 'A020501', 'A020502', 'A020503', 'A020601', 'A020602', 'A020603', 'A020604', 'A020605', 'A020606', 'A020607', 'A020608', 'A020701', 'A020702', 'A020703', 'A020801', 'A020802', 'A020803', 'A020901', 'A020902', 'A021001', 'A021002', 'A021003', 'A021004', 'A021005', 'A021006', 'A021101', 'A021102', 'A021201', 'A021202', 'A021203', 'A021204', 'A021301', 'A021302', 'A021303', 'A021304', 'A021305', 'A021306', 'A021307', 'A021308', 'A030101', 'A030102', 'A030103', 'A030201', 'A030202', 'A030203', 'A030204', 'A030205', 'A030301', 'A030302', 'A030303', 'A030304', 'A030401', 'A030402', 'A030403', 'A030404', 'A030405', 'A030406', 'A030501', 'A030502', 'A030503', 'A030504', 'A030505', 'A030506', 'A030507', 'A030508', 'A030601', 'A030602', 'A030603', 'A030604', 'A030605', 'A030606', 'A030607', 'A030608', 'A030701', 'A030702', 'A030703', 'A030704', 'A030705', 'A030801', 'A030802', 'A030803', 'A030804', 'A030805', 'A030806', 'A030807', 'A030808', 'A030901', 'A031001', 'A031002', 'A031003', 'A031101', 'A031102', 'A031103', 'A031201', 'A031202', 'A031301', 'A031302', 'A031303', 'A031304', 'A031305', 'A031306', 'A031307', 'A031308', 'A040101', 'A040102', 'A040103', 'A040104', 'A040201', 'A040202', 'A040203', 'A040204', 'A040205', 'A040301', 'A040302', 'A040303', 'A040401', 'A040402', 'A040403', 'A040404', 'A040405', 'A040406', 'A040407', 'A040501', 'A040502', 'A040503', 'A040504', 'A040505', 'A040601', 'A040602', 'A040603', 'A040604', 'A040605', 'A040701', 'A040702', 'A040703', 'A040704', 'A040801', 'A040802', 'A040803', 'A040804', 'A040805', 'A040806', 'A040807', 'A040808', 'A040809', 'A040810', 'A040901', 'A040902', 'A041001', 'A041002', 'A041003', 'A041004', 'A041101', 'A041102', 'A041103', 'A041104', 'A041105', 'A041106', 'A041107', 'A050101', 'A050102', 'A050201', 'A050202', 'A050203', 'A050204', 'A050205', 'A050206', 'A050207', 'A050208', 'A050209', 'A050210', 'A050301', 'A050302', 'A050303', 'A050401', 'A050402', 'A050403', 'A050404', 'A050501', 'A050502', 'A050503', 'A050504', 'A050505', 'A050506', 'A050507', 'A050601', 'A050602', 'A050603', 'A050604', 'A050605', 'A050606', 'A050701', 'A050702', 'A050703', 'A050704', 'A050801', 'A050802', 'A050901', 'A050902', 'A051001', 'A051002', 'A051003', 'A051004', 'A051005', 'A051006', 'A051007', 'A051008', 'A051009', 'A051010', 'A051011', 'A051101', 'A051102', 'A051103', 'A051201', 'A051202', 'A051203', 'A051204', 'A051205', 'A051206', 'A051207', 'A051208', 'CP0101001', 'CP0101002', 'CP0101003', 'CP0101004', 'CP0101005', 'CP0101006', 'CP0101007', 'CP0101008', 'CP0101009', 'CP0101010', 'CP0101011', 'CP0101012', 'CP0101013', 'CP0101014', 'CP0101015', 'CP0101016', 'CP0101017', 'CP0101018', 'CP0101019', 'CP0101020', 'CP0101021', 'CP0101022', 'CP0101023', 'CP0101024', 'CP0101025', 'CP0101026', 'CP0101027', 'CP0101028', 'CP0101029', 'CP0101030', 'CP0101031', 'CP0101032', 'CP0101033', 'CP0101034', 'CP0101035', 'CP0101036', 'CP0101037', 'CP0102001', 'CP0102002', 'CP0102003', 'CP0102004', 'CP0102005', 'CP0102006', 'CP0102007', 'CP0102008', 'CP0102009', 'CP0102010', 'CP0102011', 'CP0102012', 'CP0102013', 'CP0102014', 'CP0102015', 'CP0102016', 'CP0102017', 'CP0102018', 'CP0102019', 'CP0102020', 'CP0102021', 'CP0102022', 'CP0102023', 'CP0102024', 'CP0102025', 'CP0102026', 'CP0102027', 'CP0102028', 'CP0102029', 'CP0102030', 'CP0102031', 'CP0102032', 'CP0102033', 'CP0102034', 'CP0102035', 'CP0103001', 'CP0103002', 'CP0103003', 'CP0103004', 'CP0103005', 'CP0103006', 'CP0103007', 'CP0103008', 'CP0103009', 'CP0103010', 'CP0103011', 'CP0103012', 'CP0103013', 'CP0103014', 'CP0103015', 'CP0103016', 'CP0103017', 'CP0103018', 'CP0103019', 'CP0103020', 'CP0103021', 'CP0103022', 'CP0103023', 'CP0103024', 'CP0103025', 'XW0101001', 'XW0101002', 'XW0101003', 'XW0101004', 'XW0101005', 'XW0101006', 'XW0101007', 'XW0101008', 'XW0101009', 'XW0101010', 'XW0101011', 'XW0101012', 'XW0101013', 'XW0101014', 'XW0101015', 'XW0101016', 'XW0102001', 'XW0102002', 'XW0102003', 'XW0102004', 'XW0102005', 'XW0102006', 'XW0102007', 'XW0102008', 'XW0103001', 'XW0103002', 'XW0103003', 'XW0103004', 'XW0103005', 'XW0103006', 'XW0103007', 'XW0103008', 'XW0103009', 'XW0103010', 'XW0103011', 'XW0103012', 'XW0103013', 'XW0103014', 'XW0103015', 'XW0103016', 'XW0103017', 'XW0103018', 'XW0103019', 'XW0103020', 'XW0103021', 'XW0103022', 'XW0103023', 'XW0103024', 'XW0103025', 'XW0103026', 'XW0103027', 'XW0103028', 'XW0103029', 'XW0103030', 'J70101001', 'J70102001', 'J70102002', 'J70102003', 'J70102004', 'J70102005', 'J70102006', 'J70103001', 'J70103002', 'J70103003', 'J70103004', 'J70103005', 'J70103006', 'J70103007', 'J70103008', 'J70103009', 'J70103010', 'J70103011', 'J70103012', 'J70103013', 'J70103014', 'J70103015', 'J70103016', 'J70103017', 'J70103018', 'J70103019', 'J70103020', 'J70103021', 'J70103022', 'J70103023', 'J70103024', 'J70103025', 'J70103026', 'J70103027', 'J70103028', 'J70103029']\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "print(train_pd.head(5))\n",
    "print(predict_pd.head(5))\n",
    "label_list = [i1 for i1 in dict_pd[\"_id\"]]\n",
    "print(label_list)\n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 目录定义查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "path = '../data/'\n",
    "inp = 'input/testproject/'\n",
    "oup = 'output/testproject/'\n",
    "logp = 'logs/testproject/'\n",
    "modp = 'models/testproject/'\n",
    "\n",
    "# print(check_output([\"ls\", path + inp]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "# 训练数据总集\n",
    "data_all_file = path + inp + 'sematic_label_train.csv'\n",
    "\n",
    "# 训练数据集\n",
    "train_file = path + inp + 'train.csv'\n",
    "# 测试数据集\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "test_file = predict_pd\n",
    "\n",
    "TXT_DATA_FILE = path + inp + 'sematic_train.txt'\n",
    "XLSX_DATA_FILE = path + inp + 'sematic_train.xlsx'\n",
    "CSV_DATA_FILE = path + inp + 'sematic_train.csv'\n",
    "\n",
    "# 结果文件\n",
    "res_file = = os.path.join(\"..\", \"data\", \"thinking2\", \"baseline.csv\")\n",
    "# 模型文件\n",
    "model_path = path + modp + 'model_t.h5'\n",
    "out_path = path + oup + 'baseline.csv'\n",
    "tensor_path = path + logp + 'baseline.csv'\n",
    "\n",
    "# model_path = './model/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据集 训练转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate to 1 hot\n",
    "# 去非nan\n",
    "data_all = pd.read_csv(data_all_file)\n",
    "data_all = data_all[~(data_all[\"评论内容\"].isnull())]\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values\n",
    "\n",
    "# 变成one hot\n",
    "data_all['postive']=0\n",
    "data_all['neutral']=0\n",
    "data_all['negative']=0\n",
    "data_all.loc[data_all['分类'] == 1, 'postive'] = 1\n",
    "data_all.loc[data_all['分类'] == 0, 'neutral'] = 1\n",
    "data_all.loc[data_all['分类'] == -1, 'negative'] = 1\n",
    "\n",
    "# # 拆分测试集,训练时跳过  *******************************\n",
    "# train_all = data_all.sample(frac=1).reset_index(drop=True)\n",
    "# lenth_train = train_all.shape[0]\n",
    "# spint = int(1*lenth_train)\n",
    "# train = train_all.loc[0:spint,:]\n",
    "# test = train_all.loc[spint:,:]\n",
    "# train.to_csv(train_file, index=False)\n",
    "# test.to_csv(test_file, index=False)\n",
    "# # 拆分测试集,训练时跳过  *******************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features = 999999\n",
    "max_features = 20000\n",
    "maxlen = 300\n",
    "memn = 200\n",
    "dropn = 0.5\n",
    "\n",
    "# train = pd.read_csv(train_file)\n",
    "train = train_pd\n",
    "print(train.shape)\n",
    "# test = pd.read_csv(test_file)\n",
    "test = predict_pdint\n",
    "print(test.shape)\n",
    "for i1 in train.index:\n",
    "    train.loc[i1, \"text\"]=\" \".join(jieba.cut(train.loc[i1, \"text\"]))\n",
    "for i1 in test.index:\n",
    "    test.loc[i1, \"Description\"]=\" \".join(jieba.cut(test.loc[i1, \"Description\"]))\n",
    "    \n",
    "list_sentences_train = train[\"text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"Description\"].fillna(\"CVxTz\").values\n",
    "# list_classes = [i1 for i1 in train.columns]\n",
    "list_classes = label_list\n",
    "# list_classes = [\"negative\",\"neutral\",\"postive\"]\n",
    "try:\n",
    "    list_classes.remove(\"评论内容\")\n",
    "    list_classes.remove(\"id\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "y = train[list_classes].values\n",
    "\n",
    "# list_sentences_test = test[\"评论内容\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way attention\n",
    "# def attention_3d_block(inputs):\n",
    "#     #input_dim = int(inputs.shape[2])\n",
    "# #     a = Permute((2, 1))(inputs)\n",
    "#     x = Reshape((maxlen * ,), name='reshape_1')(x)\n",
    "#     a_probs = Dense(maxlen, activation='softmax')(inputs)\n",
    "# #     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "#     return output_attention_mul\n",
    "\n",
    "# lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm')(drop1)\n",
    "# attention_mul = attention_3d_block(lstm_out)\n",
    "# attention_flatten = Flatten()(attention_mul)\n",
    "# drop2 = Dropout(0.3)(attention_flatten)\n",
    "\n",
    "def get_model(memn,dropn):\n",
    "    embed_size = 105\n",
    "    embed_size = 300\n",
    "    # 时间步 = maxlen\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    # memory units = 50\n",
    "    x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "# #     x = GlobalMaxPool1D()(x)\n",
    "# #     x = Reshape((maxlen * memn,), name='reshape_1')(x)\n",
    "#     a_probs = Dense(maxlen, activation='softmax')(x)\n",
    "# #     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     x = multiply([x, a_probs], name='attention_mul')\n",
    "\n",
    "# #     x = attention_3d_block(x)\n",
    "#     x = Dropout(dropn)(x)\n",
    "#     x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(memn, activation=\"relu\")(x)\n",
    "    x = Dropout(dropn)(x)\n",
    "    x = Dense(3, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early, tensorb] #early\n",
    "\n",
    "# model = get_model(200, 0.2)\n",
    "model = get_model(memn, dropn)\n",
    "print(model.summary())\n",
    "# mlist = list(range(1,7))\n",
    "# droplist = list(range(1,7))\n",
    "# for memn in mlist:\n",
    "#     for dropn in droplist:\n",
    "#         model = get_model(memn*50, dropn*0.1)\n",
    "#         start = time.time()\n",
    "#         model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "#         end = time.time()\n",
    "#         print(end-start)\n",
    "#         print(memn*50, dropn*0.1)\n",
    "#         print(\"*\".center(100,\"#\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "# predict(self, x, batch_size=32, verbose=0)\n",
    "# predict_classes(self, x, batch_size=32, verbose=1)\n",
    "# predict_proba(self, x, batch_size=32, verbose=1)\n",
    "# evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "\n",
    "# test_file = path + inp + 'test_alll.csv'\n",
    "sample_submission = pd.read_csv(test_file)\n",
    "sample_submission[\"分类\"]=0\n",
    "sample_submission[\"postive\"]=0\n",
    "sample_submission[\"neutral\"]=0\n",
    "sample_submission[\"negative\"]=0\n",
    "print(train.head())\n",
    "print(sample_submission.head())\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission[\"max\"]=sample_submission[list_classes].max(axis=1)\n",
    "\n",
    "for indexs in sample_submission.index:  \n",
    "    for  i2 in list_classes:  \n",
    "        if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "            sample_submission.loc[indexs,\"predict\"]=i2\n",
    "for i1 in list_classes:\n",
    "    sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "sample_submission.to_csv(res_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 正确率评估\n",
    "score = model.evaluate(X_t, y, batch_size=batch_size)\n",
    "print(score)\n",
    "test_pd = pd.read_csv(test_file)\n",
    "res_pd = pd.read_csv(res_file)\n",
    "total_pd=pd.concat([res_pd,test_pd], join='outer', axis=1)\n",
    "total_right=0\n",
    "total_num=0\n",
    "for i1 in list_classes:\n",
    "    tmp_obj=total_pd[total_pd[i1] == 1]\n",
    "    sum_num=tmp_obj.shape[0]\n",
    "    right_num=tmp_obj[tmp_obj[\"predict\"] == i1].shape[0]\n",
    "    total_right += right_num\n",
    "    total_num += sum_num\n",
    "    try:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, accuracy: %.3f\" % (i1,sum_num,right_num,right_num/sum_num))\n",
    "    except Exception as e:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, error: %s\" % (i1,sum_num,right_num,str(e)))\n",
    "        \n",
    "print(\"total data, total_num: %d, total_right: %d, accuracy: %.3f\" % (total_num,total_right,total_right/total_num))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
