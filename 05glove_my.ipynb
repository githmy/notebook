{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Improved LSTM baseline\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/mla36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "import jieba\n",
    "import glob\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline.csv\n",
      "rasa_baseline.csv\n",
      "rasa_test.csv\n",
      "rasa_train.csv\n",
      "rasa_train_sample_submission.csv\n",
      "rasa_zh_root.json\n",
      "sample_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '../input/'\n",
    "comp = 'toxic/'\n",
    "EMBEDDING_FILE = f'{path}wordvector/wiki.zh.vec'\n",
    "TRAIN_DATA_FILE = f'{path}{comp}rasa_train.csv'\n",
    "TEST_DATA_FILE = f'{path}{comp}rasa_test.csv'\n",
    "tensor_path = \"../logs/toxic/\"\n",
    "model_path = \"../model/toxic/rasa_weights_base.best.hdf5\"\n",
    "res_file = \"../result/toxic/rasa_baseline.csv\"\n",
    "\n",
    "print(check_output([\"ls\", path + comp]).decode(\"utf8\"))\n",
    "\n",
    "jieba_userdicts = glob.glob(path + \"jieba/*.txt\")\n",
    "for jieba_userdict in jieba_userdicts:\n",
    "    jieba.load_userdict(jieba_userdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/mla36/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# 使用原始文件 分出验证集\n",
    "train_all = pd.read_csv(TRAIN_DATA_FILE)\n",
    "train_all = train_all.sample(frac=1).reset_index(drop=True)  \n",
    "lenth_train = train_all.shape[0]\n",
    "spint = int(0.8*lenth_train)\n",
    "train = train_all.loc[0:spint,:]\n",
    "test = train_all.loc[spint:,:]\n",
    "test.to_csv(TEST_DATA_FILE, index=False)\n",
    "# test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "for i1 in train.index:\n",
    "    train.loc[i1, \"comment_text\"]=\" \".join(jieba.cut(train.loc[i1, \"comment_text\"]))\n",
    "for i1 in test.index:\n",
    "    test.loc[i1, \"comment_text\"]=\" \".join(jieba.cut(test.loc[i1, \"comment_text\"]))\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_classes = [i1 for i1 in train.columns]\n",
    "try:\n",
    "    list_classes.remove(\"comment_text\")\n",
    "    list_classes.remove(\"id\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "for o in list(embeddings_index.keys()):\n",
    "     if len(embeddings_index[o])!=embed_size:\n",
    "         del embeddings_index[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b"
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)+2)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[0]=np.zeros((embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(30, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda2/envs/mla36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "epochs=100\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early, tensorb] #early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1332 samples, validate on 148 samples\n",
      "Epoch 1/100\n",
      "1332/1332 [==============================] - 33s 25ms/step - loss: 0.4959 - acc: 0.7600 - val_loss: 0.2099 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20992, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 2/100\n",
      "1332/1332 [==============================] - 30s 22ms/step - loss: 0.1661 - acc: 0.9636 - val_loss: 0.1389 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20992 to 0.13889, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 3/100\n",
      "1332/1332 [==============================] - 32s 24ms/step - loss: 0.1435 - acc: 0.9666 - val_loss: 0.1350 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13889 to 0.13501, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 4/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.1399 - acc: 0.9665 - val_loss: 0.1328 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13501 to 0.13278, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 5/100\n",
      "1332/1332 [==============================] - 30s 22ms/step - loss: 0.1374 - acc: 0.9665 - val_loss: 0.1303 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13278 to 0.13032, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 6/100\n",
      "1332/1332 [==============================] - 30s 23ms/step - loss: 0.1348 - acc: 0.9666 - val_loss: 0.1275 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13032 to 0.12753, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 7/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.1292 - acc: 0.9666 - val_loss: 0.1237 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12753 to 0.12372, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 8/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.1243 - acc: 0.9671 - val_loss: 0.1196 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12372 to 0.11961, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 9/100\n",
      "1332/1332 [==============================] - 30s 23ms/step - loss: 0.1201 - acc: 0.9676 - val_loss: 0.1140 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11961 to 0.11401, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 10/100\n",
      "1332/1332 [==============================] - 30s 23ms/step - loss: 0.1139 - acc: 0.9690 - val_loss: 0.1128 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11401 to 0.11276, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 11/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.1089 - acc: 0.9694 - val_loss: 0.1069 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.11276 to 0.10694, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 12/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.1042 - acc: 0.9705 - val_loss: 0.1041 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10694 to 0.10412, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 13/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0997 - acc: 0.9712 - val_loss: 0.1012 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.10412 to 0.10116, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 14/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0968 - acc: 0.9720 - val_loss: 0.0990 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.10116 to 0.09900, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 15/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0928 - acc: 0.9731 - val_loss: 0.0980 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09900 to 0.09801, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 16/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0875 - acc: 0.9739 - val_loss: 0.0939 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09801 to 0.09392, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 17/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0845 - acc: 0.9750 - val_loss: 0.0913 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09392 to 0.09125, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 18/100\n",
      "1332/1332 [==============================] - 26s 20ms/step - loss: 0.0819 - acc: 0.9754 - val_loss: 0.0891 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09125 to 0.08914, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 19/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0824 - acc: 0.9749 - val_loss: 0.0886 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08914 to 0.08862, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 20/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0772 - acc: 0.9769 - val_loss: 0.0880 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08862 to 0.08804, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 21/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0740 - acc: 0.9776 - val_loss: 0.0840 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08804 to 0.08399, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 22/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0709 - acc: 0.9781 - val_loss: 0.0834 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08399 to 0.08338, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 23/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0676 - acc: 0.9796 - val_loss: 0.0851 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0662 - acc: 0.9801 - val_loss: 0.0802 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08338 to 0.08024, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 25/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0630 - acc: 0.9804 - val_loss: 0.0784 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08024 to 0.07839, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 26/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0601 - acc: 0.9811 - val_loss: 0.0770 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07839 to 0.07695, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 27/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0591 - acc: 0.9817 - val_loss: 0.0778 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0558 - acc: 0.9827 - val_loss: 0.0765 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07695 to 0.07648, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 29/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0545 - acc: 0.9828 - val_loss: 0.0772 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0534 - acc: 0.9830 - val_loss: 0.0734 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07648 to 0.07339, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 31/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0731 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.07339 to 0.07314, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 32/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0492 - acc: 0.9841 - val_loss: 0.0750 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0466 - acc: 0.9854 - val_loss: 0.0753 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0464 - acc: 0.9855 - val_loss: 0.0707 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07314 to 0.07068, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0444 - acc: 0.9855 - val_loss: 0.0740 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0427 - acc: 0.9856 - val_loss: 0.0716 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0402 - acc: 0.9867 - val_loss: 0.0713 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      "1332/1332 [==============================] - 26s 20ms/step - loss: 0.0393 - acc: 0.9870 - val_loss: 0.0701 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.07068 to 0.07014, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 39/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0376 - acc: 0.9876 - val_loss: 0.0710 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0365 - acc: 0.9877 - val_loss: 0.0712 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0344 - acc: 0.9890 - val_loss: 0.0704 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0353 - acc: 0.9891 - val_loss: 0.0700 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.07014 to 0.07001, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 43/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0330 - acc: 0.9892 - val_loss: 0.0684 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.07001 to 0.06842, saving model to ../model/toxic/rasa_weights_base.best.hdf5\n",
      "Epoch 44/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0306 - acc: 0.9906 - val_loss: 0.0732 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0304 - acc: 0.9902 - val_loss: 0.0696 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0292 - acc: 0.9904 - val_loss: 0.0696 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0279 - acc: 0.9906 - val_loss: 0.0719 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0279 - acc: 0.9914 - val_loss: 0.0702 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.0712 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0696 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0697 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0239 - acc: 0.9927 - val_loss: 0.0698 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0695 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/100\n",
      "1332/1332 [==============================] - 29s 21ms/step - loss: 0.0229 - acc: 0.9927 - val_loss: 0.0715 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0711 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0226 - acc: 0.9928 - val_loss: 0.0705 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "1332/1332 [==============================] - 27s 20ms/step - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0726 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "1332/1332 [==============================] - 30s 22ms/step - loss: 0.0203 - acc: 0.9939 - val_loss: 0.0720 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0743 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0768 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "1332/1332 [==============================] - 29s 21ms/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0744 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "1332/1332 [==============================] - 28s 21ms/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.0730 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "1332/1332 [==============================] - 29s 22ms/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0710 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55fc458e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [],
   "source": [
    "# y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "# sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "# sample_submission[list_classes] = y_test\n",
    "# sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "# predict(self, x, batch_size=32, verbose=0)\n",
    "# predict_classes(self, x, batch_size=32, verbose=1)\n",
    "# predict_proba(self, x, batch_size=32, verbose=1)\n",
    "# evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "\n",
    "sample_submission = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission[\"max\"]=sample_submission[list_classes].max(axis=1)\n",
    "\n",
    "for indexs in sample_submission.index:  \n",
    "    for  i2 in list_classes:  \n",
    "        if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "            sample_submission.loc[indexs,\"predict\"]=i2\n",
    "for i1 in list_classes:\n",
    "    sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "res_file = \"../result/toxic/rasa_baseline.csv\"\n",
    "sample_submission.to_csv(res_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480/1480 [==============================] - 10s 7ms/step\n",
      "[0.026620823446963284, 0.9922522580301439]\n",
      "weather, sum_num: 23, right_num: 16, accuracy: 0.696\n",
      "p2p, sum_num: 8, right_num: 2, accuracy: 0.250\n",
      "navigation, sum_num: 18, right_num: 13, accuracy: 0.722\n",
      "travel, sum_num: 35, right_num: 25, accuracy: 0.714\n",
      "memorandum, sum_num: 5, right_num: 2, accuracy: 0.400\n",
      "new_schedule, sum_num: 25, right_num: 23, accuracy: 0.920\n",
      "communication, sum_num: 14, right_num: 13, accuracy: 0.929\n",
      "choose, sum_num: 2, right_num: 0, accuracy: 0.000\n",
      "others, sum_num: 21, right_num: 11, accuracy: 0.524\n",
      "order_food, sum_num: 52, right_num: 28, accuracy: 0.538\n",
      "news, sum_num: 16, right_num: 10, accuracy: 0.625\n",
      "medical_consultation, sum_num: 5, right_num: 1, accuracy: 0.200\n",
      "hospital_register, sum_num: 6, right_num: 6, accuracy: 1.000\n",
      "stock, sum_num: 39, right_num: 33, accuracy: 0.846\n",
      "express, sum_num: 5, right_num: 3, accuracy: 0.600\n",
      "movie, sum_num: 1, right_num: 0, accuracy: 0.000\n",
      "joke, sum_num: 5, right_num: 5, accuracy: 1.000\n",
      "history_today, sum_num: 2, right_num: 0, accuracy: 0.000\n",
      "horoscope, sum_num: 5, right_num: 3, accuracy: 0.600\n",
      "cookbook, sum_num: 13, right_num: 5, accuracy: 0.385\n",
      "music, sum_num: 11, right_num: 7, accuracy: 0.636\n",
      "violation_lookup, sum_num: 4, right_num: 4, accuracy: 1.000\n",
      "chinese_zodiac, sum_num: 9, right_num: 6, accuracy: 0.667\n",
      "homemaking, sum_num: 10, right_num: 7, accuracy: 0.700\n",
      "fitness, sum_num: 2, right_num: 2, accuracy: 1.000\n",
      "video, sum_num: 12, right_num: 5, accuracy: 0.417\n",
      "game, sum_num: 4, right_num: 2, accuracy: 0.500\n",
      "read, sum_num: 6, right_num: 2, accuracy: 0.333\n",
      "translation_language, sum_num: 4, right_num: 1, accuracy: 0.250\n",
      "exchange_rate, sum_num: 8, right_num: 6, accuracy: 0.750\n",
      "total data, total_num: 370, total_right: 241, accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "# 正确率评估\n",
    "score = model.evaluate(X_t, y, batch_size=batch_size)\n",
    "print(score)\n",
    "test_pd = pd.read_csv(TEST_DATA_FILE)\n",
    "res_pd = pd.read_csv(res_file)\n",
    "total_pd=pd.concat([res_pd,test_pd], join='outer', axis=1)\n",
    "total_right=0\n",
    "total_num=0\n",
    "for i1 in list_classes:\n",
    "    tmp_obj=total_pd[total_pd[i1] == 1]\n",
    "    sum_num=tmp_obj.shape[0]\n",
    "    right_num=tmp_obj[tmp_obj[\"predict\"] == i1].shape[0]\n",
    "    total_right += right_num\n",
    "    total_num += sum_num\n",
    "    try:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, accuracy: %.3f\" % (i1,sum_num,right_num,right_num/sum_num))\n",
    "    except Exception as e:\n",
    "        print(\"%s, sum_num: %d, right_num: %d, error: %s\" % (i1,sum_num,right_num,str(e)))\n",
    "        \n",
    "print(\"total data, total_num: %d, total_right: %d, accuracy: %.3f\" % (total_num,total_right,total_right/total_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}