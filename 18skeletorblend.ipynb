{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "# !pip install pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('max_columns', None)\n",
    "# pd.set_option('max_rows', None)\n",
    "pd.set_option('float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "# t0 方程 \n",
    "# a = k(q,mc)^2\n",
    "# b = 2*L*k(q,mc)^2 -dis*cos(phi)*sin(thmc)\n",
    "# c = L^2 * k(q,mc)^2 + dis^2 - dis*cos(phi)*sin(thmc)*L - (t+L)^2 \n",
    "# t0 = (-b +- (b^2 - 4*a*c)^(1/2))/2*a\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')\n",
    "\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fsinthmc_v'] = 1.0/data['fsinthmc']\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "data['fcosphi_v'] = 1.0/data['fcosphi']\n",
    "\n",
    "data['fcosthmc'] = np.cos(data['thetamc'])\n",
    "data['fcosthmc_v'] = 1.0/data['fcosthmc']\n",
    "data['fsinphi'] = np.sin(data['fphi'])\n",
    "data['fsinphi_v'] = 1.0/data['fsinphi']\n",
    "\n",
    "data['ftanphi'] = np.tan(data['fphi'])\n",
    "data['ftanphi_v'] = 1.0/data['ftanphi']\n",
    "data['ftanthmc'] = np.tan(data['thetamc'])\n",
    "data['ftanthmc_v'] = 1.0/data['ftanthmc']\n",
    "\n",
    "\n",
    "# data['ft2'] = data['t'] ** 2\n",
    "# data['fdis2'] = data['fdis'] ** 2\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['terror_v'] = 1.0 / data['terror']\n",
    "data['terror_v2'] =data['terror_v'] ** 2 \n",
    "data['fttrue_v'] = 1.0 / data['fttrue']\n",
    "data['fttrue2'] = data['fttrue'] ** 2\n",
    "data['fttrue2_v'] = 1.0 / data['fttrue2'] \n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "data['nhitratio_v'] = data['nhitratio']\n",
    "data['energymc_v'] = 1.0 / data['energymc']\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "data['fenergymc2_v'] = 1.0 / data['fenergymc2'] \n",
    "# data['q_v'] = 1.0 / data['q']\n",
    "data['q2'] = data['q']\n",
    "# data['q2_v'] = 1.0 / data['q2']\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n",
    "del data['xcmc']\n",
    "del data['ycmc']\n",
    "del data['fphi']\n",
    "del data['phimc']\n",
    "del data['nhitreal']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_min\"] = data.groupby([\"event_id\"])[\"t\"].min()\n",
    "info_new[\"ft_max\"] = data.groupby([\"event_id\"])[\"t\"].max()\n",
    "info_new[\"t_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"t_mean\"]\n",
    "info_new[\"ft_mean\"] = (info_new['t_mean']-info_new['ft_min']) / (info_new['ft_max']-info_new['ft_min'])\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "data = data.sort_values(by=['event_id', 'ft_rel']).reset_index(drop=True)\n",
    "for i in [4, 6, 8, 10, 12]:\n",
    "# for i in [7, 8, 11,17, 47]:\n",
    "    data[f'ft_{i}diff'] = data.groupby('event_id')['ft_rel'].diff(periods=i).fillna(0)\n",
    "    \n",
    "data['fdis_rel'] = data['fdis'] / data['fdis_mean']\n",
    "data['fdis_rel_std'] = data['fdis_rel'] / data['fdis_std']\n",
    "data['fdis2_rel'] = data['fdis_rel'] ** 2\n",
    "data['fdis2_rel_std'] = data['fdis_rel_std'] ** 2\n",
    "\n",
    "data['fcossin'] = data['fcosphi'] * data['fsinthmc']\n",
    "data['fdis_relcossin'] =  data['fdis_rel'] * data['fcossin']\n",
    "data['fdis2_relcossin'] = data['fdis2_rel'] * data['fcossin']\n",
    "data['ft_rel_errcoscos'] = data['ft_relcoscos'] * data['terror']\n",
    "data['fdis2_rel_stdcoscos'] = data['fdis2_rel_std'] * data['fcoscos']\n",
    "data['ft2_relcoscos'] = data['ft2_rel'] * data['fcoscos']\n",
    "\n",
    "data['fdis_t_relcossin'] = data['fdis_relcossin'] / data['ft_rel']\n",
    "data['fdis_t_relcossin'].fillna(0, inplace=True)\n",
    "data['fdis-t_relcossin'] = data['fdis_relcossin'] * data['ft_rel']\n",
    "\n",
    "del data['fsinsin']\n",
    "del data['fsinphi']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainshape[0])\n",
    "print(data.shape)\n",
    "testpd = data[data.flag.isna()].reset_index()\n",
    "# data.loc[data.flag.isna() & (data.t < -900), 'flag'] = 0\n",
    "# data.loc[data.flag.isna() & ((data.t > 1850) | (data.q < 0)), 'flag'] = 1\n",
    "trainpd = data[data.flag.notna()].reset_index()\n",
    "trainpd['flag'] = trainpd['flag'].astype('int')\n",
    "# trainpd = data[:trainshape[0]].reset_index()\n",
    "# testpd = data[trainshape[0]:].reset_index()\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainpd.columns)\n",
    "feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "# feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'event_id']]\n",
    "labels = trainpd['flag']\n",
    "\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"xgb\"==\"xgb\":\n",
    "    n_splits = 2\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=4399)\n",
    "    fy_submission = np.zeros(testshape[0])\n",
    "#     y_pp_xgb_stacking = np.zeros(len(labels))\n",
    "    for train_index, test_index in kf.split(trainpd):\n",
    "        print(\">>>\", train_index)\n",
    "        start_time = time.time()\n",
    "        clf = xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.05, verbosity=3,\n",
    "                                min_child_weight=1,colsample_bytree=0.7,subsample=1,\n",
    "                                eval_metric='auc', n_estimators=5000)\n",
    "#                                 eval_metric='auc', n_estimators=2000, predictor='cpu_predictor')\n",
    "        clf.fit(\n",
    "            trainpd[feature].iloc[train_index], labels[train_index],\n",
    "            eval_set=[(trainpd[feature].iloc[train_index], labels[train_index]),\n",
    "                      (trainpd[feature].iloc[test_index], labels[test_index])],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=True,\n",
    "        )\n",
    "        fig,ax = plt.subplots(figsize=(15,15))\n",
    "        for i1 in sorted({icon[0]:icon[1]for icon in zip(feature, clf.feature_importances_)}.items(), key=lambda x: -x[1]):\n",
    "            print(i1)\n",
    "        xgb.plot_importance(clf, height=0.5,max_num_features=None,ax=ax,importance_type='gain')\n",
    "        plt.show()\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        y_pred = clf.predict(trainpd[feature].iloc[test_index])\n",
    "        y_predprob = clf.predict_proba(trainpd[feature].iloc[test_index])[:, 1]\n",
    "#         y_pp_xgb_stacking[test_index] = y_predprob\n",
    "\n",
    "        auc = metrics.roc_auc_score(labels[test_index], y_predprob)\n",
    "        print(\"AUC Score (Train): %f\" % auc)\n",
    "\n",
    "        fy_submission += clf.predict_proba(testpd[feature])[:, 1] / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lgbt\"!=\"lgbt\":\n",
    "    def run_lgb(df_train, labels, df_test, use_features):\n",
    "        target = 'flag'\n",
    "        oof_pred = np.zeros((len(df_train),))\n",
    "        y_pred = np.zeros((len(df_test),))\n",
    "        folds = GroupKFold(n_splits=2)  # 6 折比 5 折好一点, 当然有时间有机器可以试下更多的 folds\n",
    "        for fold, (tr_ind, val_ind) in enumerate(folds.split(df_train, labels, df_train['event_id'])):\n",
    "            start_time = time.time()\n",
    "            print(f'Fold {fold + 1}')\n",
    "            x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind]\n",
    "            y_train, y_val = labels.iloc[tr_ind], labels.iloc[val_ind]\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            val_set = lgb.Dataset(x_val, y_val)\n",
    "#             https://www.cnblogs.com/infaraway/p/7890558.html\n",
    "            params = {\n",
    "                'learning_rate': 0.1,\n",
    "                'metric': 'auc',\n",
    "                'objective': 'binary',\n",
    "                'n_estimators': 5000,\n",
    "                'n_jobs': -1,\n",
    "                'seed': 1029,\n",
    "                'device':'gpu',\n",
    "#                 'is_unbalance': True,\n",
    "#                 过拟合\n",
    "                'max_depth': 8,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.75,\n",
    "                'bagging_freq': 2,\n",
    "#                 'min_data_in_leaf':1,\n",
    "#                 'min_sum_hessian_in_leaf':0.001,\n",
    "#                 'min_gain_to_split': 0.02,\n",
    "#                 'min_gain_to_split': 0.004,\n",
    "                'num_leaves': 64,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5, # 越小l2正则程度越高\n",
    "            }\n",
    "            model = lgb.train(params,\n",
    "                              train_set,\n",
    "                              num_boost_round=5000,\n",
    "                              early_stopping_rounds=200,\n",
    "                              valid_sets=[train_set, val_set],\n",
    "                              verbose_eval=50)\n",
    "            oof_pred[val_ind] = model.predict(x_val)\n",
    "            y_pred += model.predict(df_test[use_features]) / folds.n_splits\n",
    "\n",
    "            print(\"Features importance...\")\n",
    "            gain = model.feature_importance('gain')\n",
    "            print(gain)\n",
    "            feat_imp = pd.DataFrame({'feature': model.feature_name(),\n",
    "                                     'split': model.feature_importance('split'),\n",
    "                                     'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "            display(feat_imp)\n",
    "            used_time = (time.time() - start_time) / 3600\n",
    "            print(f'used_time: {used_time:.2f} hours')\n",
    "            del x_train, x_val, y_train, y_val, train_set, val_set\n",
    "            gc.collect()\n",
    "        return y_pred, oof_pred\n",
    "#     lentht = trainpd.shape[0]\n",
    "#     print(trainpd.shape)\n",
    "#     fy_submission, oof_pred = run_lgb(trainpd.iloc[0:lentht//100], labels.iloc[0:lentht//100], testpd, feature)\n",
    "    fy_submission, oof_pred = run_lgb(trainpd, labels, testpd, feature)\n",
    "    score = roc_auc_score(labels, oof_pred)\n",
    "    print('auc: ', score)\n",
    "    print(fy_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分一部分数据作为测试集'''\n",
    "    train1pd, train2pd ,label1s, label2s = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "    dataset_blend_1train = np.zeros((train1pd.shape[0], len(clfs)))\n",
    "    dataset_blend_2train = np.zeros((train2pd.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    '''5折stacking'''\n",
    "    n_folds = 2\n",
    "    skf = list(StratifiedKFold(label1s, n_folds))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        print(j, clf)\n",
    "        start_time = time.time() \n",
    "        dataset_blend_2train_j = np.zeros((train2pd.shape[0], len(skf)))\n",
    "        dataset_blend_test_j = np.zeros((testpd.shape[0], len(skf)))\n",
    "        for i, (train_index, test_index) in enumerate(skf):\n",
    "            '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "            # print(\"Fold\", i)\n",
    "            X_d1, y_d1, X_d2, y_d2 = train1pd[train_index], label1s[train_index], train2pd[test_index], label1s[test_index]\n",
    "            clf.fit(X_d1, y_d1)\n",
    "            dataset_blend_1train[test_index, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "            dataset_blend_2train_j[:, i] = clf.predict_proba(train2pd)[:, 1]\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "        dataset_blend_2train[:, j] = dataset_blend_2train_j.mean(1)\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(label2s, dataset_blend_2train[:, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "# clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_blend_1train, label1s)\n",
    "    y_submission = clf2.predict_proba(dataset_blend_2train)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "    print(\"Linear stretch of predictions to [0,1]\")\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "    print(\"blend result\")\n",
    "    print(\"val auc Score: %f\" % (roc_auc_score(y_predict, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "#         svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分训练数据集为d1,d2两部分'''\n",
    "    X_d1, X_d2, y_d1, y_d2 = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "#     dataset_d1 = np.zeros((X_d1.shape[0], len(clfs)))\n",
    "    dataset_d2 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "    dataset_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        print(j, clf)\n",
    "        start_time = time.time()\n",
    "        clf.fit(X_d1, y_d1)\n",
    "        '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''\n",
    "        dataset_d2[:, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值作为新的特征。'''\n",
    "        dataset_test[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "#         dataset_test[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(y_d2, dataset_d2[:, j]))\n",
    "        used_time = (time.time() - start_time) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    '''融合使用的模型'''\n",
    "    # clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_d2, y_d2)\n",
    "#     y_submission = clf2.predict_proba(dataset_d2)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_test)[:, 1]\n",
    "\n",
    "#     print(\"Linear stretch of predictions to [0,1]\")\n",
    "#     y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "#     print(\"blend result\")\n",
    "#     print(\"val auc Score: %f\" % (roc_auc_score(y_d2, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"prod\" ==\"prod\":\n",
    "    random_state = np.random.RandomState(42)\n",
    "    outliers_fraction = 0.05\n",
    "    # Define seven outlier detection tools to be compared\n",
    "    classifiers = {\n",
    "            'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n",
    "            'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n",
    "            'Feature Bagging':FeatureBagging(LOF(n_neighbors=35),contamination=outliers_fraction,check_estimator=False,random_state=random_state),\n",
    "            'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n",
    "            'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state),\n",
    "            'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n",
    "            'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n",
    "    }\n",
    "    \n",
    "    dfx = pd.DataFrame()\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        start_time = time.time()\n",
    "        print(clf_name)\n",
    "        clf.fit(X)\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(X) * -1\n",
    "\n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        y_pred = clf.predict(X)\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "    #     n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "    #     n_outliers = np.count_nonzero(y_pred == 1)\n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "        # copy of dataframe\n",
    "        dfx[clf_name] = y_pred\n",
    "\n",
    "        # IX1 - inlier feature 1,  IX2 - inlier feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.25\n",
    "\n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testpd['hit_id']\n",
    "sub['flag_pred'] = fy_submission\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}