{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import multiply\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Masking, Activation\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D \n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import time  # pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('max_columns', None)\n",
    "# pd.set_option('max_rows', None)\n",
    "pd.set_option('float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "# t0 方程 \n",
    "# a = k(q,mc)^2\n",
    "# b = 2*L*k(q,mc)^2 -dis*cos(phi)*sin(thmc)\n",
    "# c = L^2 * k(q,mc)^2 + dis^2 - dis*cos(phi)*sin(thmc)*L - (t+L)^2 \n",
    "# t0 = (-b +- (b^2 - 4*a*c)^(1/2))/2*a\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')\n",
    "\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fsinthmc_v'] = 1.0/data['fsinthmc']\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "data['fcosphi_v'] = 1.0/data['fcosphi']\n",
    "\n",
    "data['fcosthmc'] = np.cos(data['thetamc'])\n",
    "data['fcosthmc_v'] = 1.0/data['fcosthmc']\n",
    "data['fsinphi'] = np.sin(data['fphi'])\n",
    "data['fsinphi_v'] = 1.0/data['fsinphi']\n",
    "\n",
    "data['ftanphi'] = np.tan(data['fphi'])\n",
    "data['ftanphi_v'] = 1.0/data['ftanphi']\n",
    "data['ftanthmc'] = np.tan(data['thetamc'])\n",
    "data['ftanthmc_v'] = 1.0/data['ftanthmc']\n",
    "\n",
    "\n",
    "# data['ft2'] = data['t'] ** 2\n",
    "# data['fdis2'] = data['fdis'] ** 2\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['terror_v'] = 1.0 / data['terror']\n",
    "data['terror_v2'] =data['terror_v'] ** 2 \n",
    "data['fttrue_v'] = 1.0 / data['fttrue']\n",
    "data['fttrue2'] = data['fttrue'] ** 2\n",
    "data['fttrue2_v'] = 1.0 / data['fttrue2'] \n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "data['nhitratio_v'] = data['nhitratio']\n",
    "data['energymc_v'] = 1.0 / data['energymc']\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "data['fenergymc2_v'] = 1.0 / data['fenergymc2'] \n",
    "# data['q_v'] = 1.0 / data['q']\n",
    "data['q2'] = data['q']\n",
    "# data['q2_v'] = 1.0 / data['q2']\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n",
    "del data['xcmc']\n",
    "del data['ycmc']\n",
    "del data['fphi']\n",
    "del data['phimc']\n",
    "del data['nhitreal']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_min\"] = data.groupby([\"event_id\"])[\"t\"].min()\n",
    "info_new[\"ft_max\"] = data.groupby([\"event_id\"])[\"t\"].max()\n",
    "info_new[\"t_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"t_mean\"]\n",
    "info_new[\"ft_mean\"] = (info_new['t_mean']-info_new['ft_min']) / (info_new['ft_max']-info_new['ft_min'])\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "# data['ft_rel'] = (data['t']-data['ft_min']) / (data['ft_max']-data['ft_min'])\n",
    "data['ft_rel'] = data['t'] / data['ft_std']\n",
    "# data['ft_rel_std'] = data['ft_rel'] / data['ft_std']\n",
    "data['ft2_rel'] = data['ft_rel'] ** 2\n",
    "# data['ft2_rel_std'] = data['ft_rel_std'] ** 2\n",
    "data['ft_rel_v'] = 1.0 / data['ft_rel']\n",
    "data['ft2_rel_v'] = 1.0 / data['ft2_rel'] \n",
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "data = data.sort_values(by=['event_id', 'ft_rel']).reset_index(drop=True)\n",
    "for i in [1]:\n",
    "    data[f'ft_{i}diff'] = data.groupby('event_id')['ft_rel'].diff(periods=i).fillna(0)\n",
    "    \n",
    "data['fdis_rel'] = data['fdis'] / data['fdis_mean']\n",
    "\n",
    "\n",
    "data['ft_rel_errcoscos'] = data['ft_relcoscos'] * data['terror']\n",
    "data['fdis2_rel_stdcoscos'] = data['fdis2_rel_std'] * data['fcoscos']\n",
    "data['ft2_relcoscos'] = data['ft2_rel'] * data['fcoscos']\n",
    "\n",
    "del data['fsinthmc']\n",
    "del data['fcosthmc']\n",
    "del data['fsincos']\n",
    "del data['fsinsin']\n",
    "del data['fsinphi']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainshape[0])\n",
    "print(data.shape)\n",
    "\n",
    "# print(data[data['hit_id']<0])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# feature = [x for x in data.columns if x not in ['flag', 'index', 'event_id']]\n",
    "feature_e = [x for x in data.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "data[feature_e] = scaler.fit_transform(data[feature_e])\n",
    "\n",
    "evenidlist = list(set(data['event_id']))\n",
    "grouplist = [data[data['event_id'] == i1] for i1 in evenidlist]\n",
    "# print(data[data['hit_id']<0])\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 4000\n",
    "trainpd = []\n",
    "lenpd = []\n",
    "for i1 in grouplist:\n",
    "    if ~np.isnan(i1.iloc[0].flag):\n",
    "        tnp = -np.ones((timestep,len(feature_e)))\n",
    "        tnp[0:i1.shape[0],:] =  np.array(i1[feature_e])\n",
    "        trainpd.append(tnp[np.newaxis,:])\n",
    "#         trainpd.append(np.array(i1[feature_e]))\n",
    "        lenpd.append(i1.shape[0])\n",
    "trainpd = np.concatenate(trainpd, 0)\n",
    "lenpd = np.array(lenpd)\n",
    "\n",
    "labels = []\n",
    "for i1 in grouplist:\n",
    "    if ~np.isnan(i1.iloc[0].flag):\n",
    "        tnp = np.zeros((timestep))\n",
    "        tnp[0:i1.shape[0]] =  np.array(i1[\"flag\"], dtype='int64')\n",
    "        labels.append(tnp[np.newaxis,:])\n",
    "#         labels.append(np.array(i1[\"flag\"], dtype='int8'))\n",
    "labels = np.concatenate(labels, 0)\n",
    "\n",
    "testpd = []\n",
    "testhitidpd = []\n",
    "testeventidpd = []\n",
    "cenum = 0\n",
    "for i1 in grouplist:\n",
    "    if np.isnan(i1.iloc[0].flag):\n",
    "        cenum +=i1.shape[0] \n",
    "        tnp = -np.ones((timestep,len(feature_e)))\n",
    "        thidp = -np.ones((timestep), dtype='int64')\n",
    "        teidp = -np.ones((timestep), dtype='int64')\n",
    "        tnp[0:i1.shape[0],:] =  np.array(i1[feature_e])\n",
    "        testpd.append(tnp[np.newaxis,:])\n",
    "        \n",
    "        thidp[0:i1.shape[0]] =  np.array(i1[\"hit_id\"], dtype='int64')\n",
    "        testhitidpd.append(thidp[np.newaxis,:])\n",
    "        teidp[0:i1.shape[0]] =  np.array(i1[\"event_id\"], dtype='int64')\n",
    "        testeventidpd.append(teidp[np.newaxis,:])\n",
    "testpd = np.concatenate(testpd, 0)\n",
    "testhitidpd = np.concatenate(testhitidpd, 0)\n",
    "testeventidpd = np.concatenate(testeventidpd, 0)\n",
    "print(cenum)\n",
    "del grouplist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc as auccc\n",
    "keras.backend.clear_session()\n",
    "def roc_auc_score_FIXED(y_true, y_pred):\n",
    "    if len(np.unique(y_true)) == 1: # bug in roc_auc_score\n",
    "#         return accuracy_score(y_true, np.rint(y_pred))\n",
    "        return np.mean(accuracy_score(y_true, np.rint(y_pred)))\n",
    "    y_true = y_true.reshape((-1))\n",
    "    y_pred = y_pred.reshape((-1))\n",
    "    return np.mean(roc_auc_score(y_true, y_pred))*1000\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func( roc_auc_score_FIXED, (y_true, y_pred), tf.double)\n",
    "\n",
    "def mycrossentropy(y_true, y_pred):\n",
    "    loss1 = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(loss1) \n",
    "\n",
    "def get_model_cnn():    \n",
    "    embed_size = len(feature_e)\n",
    "    inp = Input(shape=[None, embed_size])\n",
    "#     x = Masking(mask_value=0.)(inp)\n",
    "    x = inp\n",
    "    filters = 30\n",
    "    x = Conv1D(filters, 1, strides=(1), padding='same')(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "#     filters = 15\n",
    "#     x = Conv1D(filters, 1, strides=(1), padding='same')(inp)\n",
    "#     x = Dense(60, activation='elu')(x)\n",
    "    for iit in range(3):\n",
    "        filters = 60*(1+iit)\n",
    "        x = Conv1D(filters, 3, strides=(1), padding='same')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Activation('elu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    filters = 30\n",
    "    x = Conv1D(filters, 1, strides=(1), padding='same')(x)\n",
    "    x = Activation('elu')(x)\n",
    "    filters = 10\n",
    "    x = Conv1D(filters, 1, strides=(1), padding='same')(x)\n",
    "    x = Activation('elu')(x)\n",
    "    filters = 1\n",
    "    x = Conv1D(filters, 1, strides=(1), padding='same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "#     x = Dense(1, activation='elu')(x)\n",
    "    def sqqueeze(x):\n",
    "        return K.squeeze(x, axis=-1)\n",
    "\n",
    "    x = keras.layers.Lambda(sqqueeze)(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(\n",
    "        loss=mycrossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=[auroc])\n",
    "    return model\n",
    "\n",
    "def get_model_lstm():\n",
    "    embed_size = len(feature_e)\n",
    "    memn = 100\n",
    "\n",
    "#     inp = Input(shape=[timestep, embed_size])\n",
    "    inp = Input(shape=[None, embed_size])\n",
    "    x = Masking(mask_value=-1.)(inp)\n",
    "#     x = Bidirectional(LSTM(memn, return_sequences=True))(inp)\n",
    "    x = Bidirectional(LSTM(memn, return_sequences=True))(x)\n",
    "    x = Dense(memn, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Bidirectional(LSTM(memn // 2, return_sequences=True))(x)\n",
    "#     x = Dense(10, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    def sqqueeze(x):\n",
    "        return K.squeeze(x, axis=-1)\n",
    "\n",
    "    x = keras.layers.Lambda(sqqueeze)(x)\n",
    "#     x = Flatten()(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(\n",
    "        loss=mycrossentropy,\n",
    "#         loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "#                   metrics=[auc_roc])\n",
    "                  metrics=[auroc])\n",
    "#                   metrics=[auc])\n",
    "#                   metrics=['accuracy', auroc])\n",
    "    return model\n",
    "\n",
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "model_path = os.path.join(pathf, \"model\", \"tmp_cnn.h5\")\n",
    "model_path = os.path.join(pathf, \"model\", \"tmp_lstm.h5\")\n",
    "log_path = os.path.join(pathf, \"model\")\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=log_path, histogram_freq=10, write_graph=True, write_images=True,\n",
    "                      embeddings_freq=0,\n",
    "                      embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=200)\n",
    "callbacks_list = [checkpoint, early, tensorb]  # early\n",
    "\n",
    "if \"lstm\" in model_path:\n",
    "    model = get_model_lstm()\n",
    "else:\n",
    "    model = get_model_cnn()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 100\n",
    "model.load_weights(model_path)\n",
    "start = time.time()\n",
    "model.fit(trainpd, labels, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks_list)\n",
    "end = time.time()\n",
    "print(\"outbatch: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(pathf, \"model\", \"tmp_lstm_1.h5\")\n",
    "model.load_weights(model_path)\n",
    "fy_submission = model.predict(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "fy_submission=fy_submission.reshape((-1))\n",
    "testhitidpd=testhitidpd.reshape((-1))\n",
    "testeventidpd=testeventidpd.reshape((-1))\n",
    "# print(set(testeventidpd))\n",
    "# 1. \n",
    "thre = 0.3\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testhitidpd\n",
    "sub['flag_pred'] = fy_submission\n",
    "sub['event_id'] = testeventidpd\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub=sub[sub['event_id']!=-1]\n",
    "\n",
    "# 2. group 为正的比率小于阈值，全置零。\n",
    "threshe = 0.15\n",
    "def filtfun(data):\n",
    "    uper = sum(data['flag_pred'])\n",
    "    anum =data.shape[0]\n",
    "    return uper/data.shape[0]\n",
    "\n",
    "filt = pd.DataFrame()\n",
    "filt[\"event_id\"] = sub.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "filt['ratioss'] = sub.groupby(['event_id']).apply(filtfun)\n",
    "filt.reset_index(drop=True, inplace=True)\n",
    "sub = pd.merge(sub, filt, on='event_id', how='left')\n",
    "sub.loc[sub['ratioss'] < threshe,\"flag_pred\"] = 0\n",
    "sub=sub[[\"hit_id\",\"flag_pred\",\"event_id\"]]\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
