{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Improved LSTM baseline\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "# from keras.models import Graph\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "import jieba\n",
    "import glob\n",
    "import json\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_lenth:  861\n"
     ]
    }
   ],
   "source": [
    "bpath = os.path.join(\"..\", \"data\")\n",
    "# EMBEDDING_FILE = os.path.join(bpath, \"wordvector\", \"crawl-300d-2M.vec\")\n",
    "EMBEDDING_FILE = os.path.join(bpath, \"wordvector\", \"wiki.zh.vec\")\n",
    "TRAIN_DATA_FILE = os.path.join(bpath, \"thinking2\", \"question_obj.csv\")\n",
    "VALID_DATA_FILE = os.path.join(bpath, \"thinking2\", \"valid_obj.csv\")\n",
    "predict_file = os.path.join(bpath, \"thinking2\",\"predict_obj.csv\")\n",
    "\n",
    "# tmpo_path = os.path.join(bpath, \"thinking2\", \"predict_obj.csv\")\n",
    "# predict_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep='\\t')\n",
    "# tmpo_path = os.path.join(bpath, \"thinking2\", \"question_obj.csv\")\n",
    "# train_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "tmpo_path = os.path.join(bpath, \"thinking2\", \"review_obj.csv\")\n",
    "dict_pd = pd.read_csv(tmpo_path, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "label_list = [i1 for i1 in dict_pd[\"_id\"]]\n",
    "label_lenth = len(label_list)\n",
    "print(\"label_lenth: \",label_lenth)\n",
    "\n",
    "jieba_userdicts = glob.glob(os.path.join(bpath, \"jieba\", \"*.txt\"))\n",
    "for jieba_userdict in jieba_userdicts:\n",
    "    jieba.load_userdict(jieba_userdict)\n",
    "    print(\"load dict:\",jieba_userdicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "\n",
    "# 使用原始文件 分出验证集\n",
    "train_all = pd.read_csv(TRAIN_DATA_FILE)\n",
    "train_all = train_all.sample(frac=1, random_state=998).reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_in_item length:  839\n",
      "rm_row_list length:  1029\n"
     ]
    }
   ],
   "source": [
    "rm_row_list = []\n",
    "not_in_label_list = []\n",
    "not_in_item = []\n",
    "np_list_r =[]\n",
    "np_list_m =[]\n",
    "for i1 in train_all.index:\n",
    "    # 1. 清理内容，转为数组\n",
    "    restt_r = train_all.loc[i1, \"reviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "    restt_m = train_all.loc[i1, \"mainReviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "#     print(i1,train.loc[i1, \"reviewPoints\"],restt)\n",
    "    # 2. 判断删除项\n",
    "    listindex_del_r = []\n",
    "    listindex_del_m = []\n",
    "    dealstrl_r = []\n",
    "    dealstrl_m = []\n",
    "    for id2,i2 in enumerate(restt_r):\n",
    "        dealstr_r = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_r == \"\" or dealstr_r == \" \":\n",
    "            listindex_del_r.append(id2)\n",
    "        elif dealstr_r not in label_list:\n",
    "            listindex_del_r.append(id2)\n",
    "#             not_in_label_list.append([train.loc[i1, \"id\"], dealstr])\n",
    "            not_in_item.append(train_all.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_r.append(dealstr_r)\n",
    "    for id2,i2 in enumerate(restt_m):\n",
    "        dealstr_m = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_m == \"\" or dealstr_m == \" \":\n",
    "            listindex_del_m.append(id2)\n",
    "        elif dealstr_m not in label_list:\n",
    "            listindex_del_m.append(id2)\n",
    "            not_in_item.append(train_all.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_m.append(dealstr_m)\n",
    "    # 3. 删除 删除项\n",
    "    listindex_del_r.reverse()\n",
    "    for i2 in listindex_del_r:\n",
    "        restt_r.pop(i2)\n",
    "    listindex_del_m.reverse()\n",
    "    for i2 in listindex_del_m:\n",
    "        restt_m.pop(i2)\n",
    "    # 4. 判断移除项\n",
    "    if len(restt_r)==0 and len(restt_m)==0 :\n",
    "        rm_row_list.append(i1)\n",
    "    else:\n",
    "        np_list_r.append(dealstrl_r)\n",
    "        np_list_m.append(dealstrl_m)\n",
    "# print(i1,len(restt))\n",
    "print(\"not_in_item length: \",len(set(not_in_item)))\n",
    "# print(not_in_item)\n",
    "print(\"rm_row_list length: \",len(rm_row_list))\n",
    "# print(rm_row_list)\n",
    "# print(np_list_r)\n",
    "\n",
    "epfile = os.path.join(\"..\",\"data\",\"error.json\")\n",
    "with open(epfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list(set(not_in_item)), f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file = os.path.join(bpath, \"thinking2\", \"review_obj.csv\")\n",
    "map_pd = pd.read_csv(map_file)\n",
    "map_points = {map_pd.loc[i1,\"_id\"]:map_pd.loc[i1,\"name\"] for i1 in map_pd.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2491, 5)\n",
      "               id  level mainReviewPoints   reviewPoints  \\\n",
      "0  RJB080B1230021      2      ['S160302']    ['S160302']   \n",
      "1  RJB080A4321001      2      ['S091401']    ['S091401']   \n",
      "2          4B4306      1      ['A041003']    ['A041003']   \n",
      "3  RJB070A3310004      2      ['S060305']    ['S060305']   \n",
      "4      ZKBCS06019      1    ['CP0102017']  ['CP0102017']   \n",
      "\n",
      "                                                text  \n",
      "0  计算： $a\\sqrt{\\frac{2b}{a}}\\div b\\sqrt{\\frac{2a}...  \n",
      "1                                   分解因式 $1-36b^{2}$  \n",
      "2                                        小练习（1）-判断对错  \n",
      "3  $y=2$ 是方程 $2-(m-y)=3y$ 的解，求关于 $x$ 的方程 $m(x+4)=...  \n",
      "4  先化简 ，再求值：$\\frac{x}{x-4}+\\frac{4}{x^{2}-16}\\div...  \n",
      "(277, 5)\n",
      "               id  level mainReviewPoints                       reviewPoints  \\\n",
      "0  RJB070B6200011      2      ['K240511']                        ['K240511']   \n",
      "1          4B1206      1      ['A040702']                        ['A040702']   \n",
      "2  RJB090B3220015      2      ['K240603']  ['K140104', 'S240602', 'K240603']   \n",
      "3      K050301009      0      ['K050301']                        ['K050301']   \n",
      "4  RJB080B1230020      2      ['S160302']                        ['S160302']   \n",
      "\n",
      "                                                text  \n",
      "0  已知数据总数是 $30$ ，在样本频数分布直方图（如下图）中，各小长方形的高之比为 $AE$...  \n",
      "1                                   整数的运算性质-填写数字和运算符  \n",
      "2  如图， $\\bigodot O$ 的半径为 $1cm$ ，$PA$ ，$PB$ 是 $\\bi...  \n",
      "3                          如果 $|x|+x=0$，那么 $(\\quad)$  \n",
      "4  计算： $2\\sqrt{x^{2}y}\\div \\sqrt{3y}\\times \\sqrt{...  \n",
      "np_list_r length:  2491\n",
      "np_list_m length:  2491\n",
      "np_list_r length:  277\n",
      "np_list_m length:  277\n"
     ]
    }
   ],
   "source": [
    "train_before_split = train_all.drop(rm_row_list).reset_index(drop=True)\n",
    "nolabel = train_all.loc[rm_row_list,:].reset_index(drop=True)\n",
    "\n",
    "# print(train_all.head())\n",
    "# print(train_all.shape)\n",
    "# print(nolabel.head())\n",
    "# print(nolabel.shape)\n",
    "# print(train_before_split.head())\n",
    "# print(train_before_split.shape)\n",
    "\n",
    "# 没有latex数据的内部转化版\n",
    "# train = train_all\n",
    "# predict_pd = pd.read_csv(predict_file, header=0, encoding=\"utf8\", dtype=str,sep='\\t')\n",
    "lenth_train = train_before_split.shape[0]\n",
    "spint = int(0.9*lenth_train)\n",
    "train = train_before_split.loc[0:spint,:].reset_index(drop=True)\n",
    "predict_pd = train_before_split.loc[spint:,:].reset_index(drop=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.head())\n",
    "print(predict_pd.shape)\n",
    "print(predict_pd.head())\n",
    "\n",
    "np_list_r =[]\n",
    "np_list_m =[]\n",
    "for i1 in train.index:\n",
    "    # 1. 清理内容，转为数组\n",
    "    restt_r = train.loc[i1, \"reviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "    restt_m = train.loc[i1, \"mainReviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "#     print(i1,train.loc[i1, \"reviewPoints\"],restt)\n",
    "    # 2. 判断删除项\n",
    "    listindex_del_r = []\n",
    "    listindex_del_m = []\n",
    "    dealstrl_r = []\n",
    "    dealstrl_m = []\n",
    "    for id2,i2 in enumerate(restt_r):\n",
    "        dealstr_r = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_r == \"\" or dealstr_r == \" \":\n",
    "            listindex_del_r.append(id2)\n",
    "        elif dealstr_r not in label_list:\n",
    "            listindex_del_r.append(id2)\n",
    "#             not_in_label_list.append([train.loc[i1, \"id\"], dealstr])\n",
    "            not_in_item.append(train.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_r.append(dealstr_r)\n",
    "    for id2,i2 in enumerate(restt_m):\n",
    "        dealstr_m = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_m == \"\" or dealstr_m == \" \":\n",
    "            listindex_del_m.append(id2)\n",
    "        elif dealstr_m not in label_list:\n",
    "            listindex_del_m.append(id2)\n",
    "            not_in_item.append(train.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_m.append(dealstr_m)\n",
    "    # 3. 删除 删除项\n",
    "    listindex_del_r.reverse()\n",
    "    for i2 in listindex_del_r:\n",
    "        restt_r.pop(i2)\n",
    "    listindex_del_m.reverse()\n",
    "    for i2 in listindex_del_m:\n",
    "        restt_m.pop(i2)\n",
    "    # 4. 判断移除项\n",
    "    if len(restt_r)==0 and len(restt_m)==0 :\n",
    "        rm_row_list.append(i1)\n",
    "    else:\n",
    "        np_list_r.append(dealstrl_r)\n",
    "        np_list_m.append(dealstrl_m)\n",
    "\n",
    "print(\"np_list_r length: \",len(np_list_r))\n",
    "print(\"np_list_m length: \",len(np_list_m))\n",
    "\n",
    "# # 5. 生成标签列\n",
    "yr = np.zeros((len(np_list_r), len(label_list)))\n",
    "ym = np.zeros((len(np_list_m), len(label_list)))\n",
    "for id1,i1 in enumerate(np_list_r):\n",
    "    for i2 in i1:\n",
    "        yr[id1,label_list.index(i2)] = 1\n",
    "for id1,i1 in enumerate(np_list_m):\n",
    "    for i2 in i1:\n",
    "        ym[id1,label_list.index(i2)] = 1\n",
    "\n",
    "np_list_r =[]\n",
    "np_list_m =[]\n",
    "for i1 in predict_pd.index:\n",
    "    # 1. 清理内容，转为数组\n",
    "    restt_r = predict_pd.loc[i1, \"reviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "    restt_m = predict_pd.loc[i1, \"mainReviewPoints\"].lstrip(\"[\").rstrip(\"]\").strip(\" \").strip(\"'\").split(\"', '\")\n",
    "#     print(i1,train.loc[i1, \"reviewPoints\"],restt)\n",
    "    # 2. 判断删除项\n",
    "    listindex_del_r = []\n",
    "    listindex_del_m = []\n",
    "    dealstrl_r = []\n",
    "    dealstrl_m = []\n",
    "    for id2,i2 in enumerate(restt_r):\n",
    "        dealstr_r = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_r == \"\" or dealstr_r == \" \":\n",
    "            listindex_del_r.append(id2)\n",
    "        elif dealstr_r not in label_list:\n",
    "            listindex_del_r.append(id2)\n",
    "#             not_in_label_list.append([train.loc[i1, \"id\"], dealstr])\n",
    "            not_in_item.append(predict_pd.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_r.append(dealstr_r)\n",
    "    for id2,i2 in enumerate(restt_m):\n",
    "        dealstr_m = i2.replace(\"\\n\",\"\").strip(\", '\").strip(\"', \")\n",
    "        if dealstr_m == \"\" or dealstr_m == \" \":\n",
    "            listindex_del_m.append(id2)\n",
    "        elif dealstr_m not in label_list:\n",
    "            listindex_del_m.append(id2)\n",
    "            not_in_item.append(predict_pd.loc[i1, \"id\"])\n",
    "        else:\n",
    "            dealstrl_m.append(dealstr_m)\n",
    "    # 3. 删除 删除项\n",
    "    listindex_del_r.reverse()\n",
    "    for i2 in listindex_del_r:\n",
    "        restt_r.pop(i2)\n",
    "    listindex_del_m.reverse()\n",
    "    for i2 in listindex_del_m:\n",
    "        restt_m.pop(i2)\n",
    "    # 4. 判断移除项\n",
    "    if len(restt_r)==0 and len(restt_m)==0 :\n",
    "        rm_row_list.append(i1)\n",
    "    else:\n",
    "        np_list_r.append(dealstrl_r)\n",
    "        np_list_m.append(dealstrl_m)\n",
    "\n",
    "print(\"np_list_r length: \",len(np_list_r))\n",
    "print(\"np_list_m length: \",len(np_list_m))\n",
    "\n",
    "# # 5. 生成标签列\n",
    "yr_pred = np.zeros((len(np_list_r), len(label_list)))\n",
    "ym_pred = np.zeros((len(np_list_m), len(label_list)))\n",
    "for id1,i1 in enumerate(np_list_r):\n",
    "    for i2 in i1:\n",
    "        yr_pred[id1,label_list.index(i2)] = 1\n",
    "for id1,i1 in enumerate(np_list_m):\n",
    "    for i2 in i1:\n",
    "        ym_pred[id1,label_list.index(i2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Smile\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.783 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id  level mainReviewPoints reviewPoints  \\\n",
      "0  RJB080B1230021      2       二次根式的乘法和除法   二次根式的乘法和除法   \n",
      "1  RJB080A4321001      2      因式分解-平方差公式法  因式分解-平方差公式法   \n",
      "2          4B4306      1           小练习（1）       小练习（1）   \n",
      "3  RJB070A3310004      2      解一元一次方程-去括号  解一元一次方程-去括号   \n",
      "4      ZKBCS06019      1            分式的计算        分式的计算   \n",
      "\n",
      "                                                text  \n",
      "0  计算 ：   $ a \\ sqrt { \\ frac { 2b } { a } } \\ di...  \n",
      "1                       分解 因 式   $ 1 - 36b ^ { 2 } $  \n",
      "2                                 小 练习 （ 1 ） - 判断 对错  \n",
      "3  $ y = 2 $   是 方程   $ 2 - ( m - y ) = 3y $   的 ...  \n",
      "4  先 化简   ， 再 求值 ： $ \\ frac { x } { x - 4 } + \\ f...  \n",
      "(2491, 1722)\n",
      "(2491, 6)\n"
     ]
    }
   ],
   "source": [
    "# print(map_points)\n",
    "for i1 in train.index:\n",
    "    train.loc[i1, \"text\"]=\" \".join(jieba.cut(train.loc[i1, \"text\"]))\n",
    "    tmpstr = \",\".join([map_points[label_list[id2]] for id2,i2 in enumerate(ym[i1,:]) if i2>0.5])\n",
    "    train.loc[i1, \"mainReviewPoints\"]=tmpstr\n",
    "    tmpstr = \",\".join([map_points[label_list[id2]] for id2,i2 in enumerate(yr[i1,:]) if i2>0.5])\n",
    "    train.loc[i1, \"reviewPoints\"]=tmpstr\n",
    "\n",
    "# 预测的值\n",
    "for i1 in predict_pd.index:\n",
    "    predict_pd.loc[i1, \"text\"]=\" \".join(jieba.cut(predict_pd.loc[i1, \"text\"]))\n",
    "    tmpstr = \",\".join([map_points[label_list[id2]] for id2,i2 in enumerate(ym_pred[i1,:]) if i2>0.5])\n",
    "    predict_pd.loc[i1, \"mainReviewPoints\"]=tmpstr\n",
    "    tmpstr = \",\".join([map_points[label_list[id2]] for id2,i2 in enumerate(yr_pred[i1,:]) if i2>0.5])\n",
    "    predict_pd.loc[i1, \"reviewPoints\"]=tmpstr\n",
    "\n",
    "for i1 in nolabel.index:\n",
    "    nolabel.loc[i1, \"text\"]=\" \".join(jieba.cut(nolabel.loc[i1, \"text\"]))\n",
    "    \n",
    "print(train.head())\n",
    "# 内侧数据不同版本\n",
    "TMP_TRAIN_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_origin.csv\")\n",
    "TMP_LABEL_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_label.csv\")\n",
    "TMP_NOLABEL_FILE = os.path.join(bpath, \"thinking2\", \"train_compare_nolabel.csv\")\n",
    "train.to_csv(TMP_TRAIN_FILE, index=False)\n",
    "predict_pd.to_csv(TMP_LABEL_FILE, index=False)\n",
    "nolabel.to_csv(TMP_NOLABEL_FILE, index=False)\n",
    "\n",
    "list_sentences_train = train[\"text\"].fillna(\"_na_\").values\n",
    "train = pd.get_dummies(train, columns=['level'])\n",
    "# 训练的标签\n",
    "yc = np.hstack((ym, yr))\n",
    "\n",
    "list_classes = [i1 for i1 in train.columns if i1.startswith(\"level_\")]\n",
    "yl = train[list_classes].values\n",
    "\n",
    "print(yc.shape)\n",
    "print(yl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'level', 'mainReviewPoints', 'reviewPoints', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# for i1 in predict_pd.index:\n",
    "#     predict_pd.loc[i1, \"Description\"]=\" \".join(jieba.cut(predict_pd.loc[i1, \"Description\"]))\n",
    "#     if i1 % 1000==0:\n",
    "#         print(i1)\n",
    "print(predict_pd.columns)\n",
    "# list_sentences_test = predict_pd[\"Description\"].fillna(\"_na_\").values\n",
    "list_sentences_test_label = predict_pd[\"text\"].fillna(\"_na_\").values\n",
    "list_sentences_test_nolabel = nolabel[\"text\"].fillna(\"_na_\").values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test_label = tokenizer.texts_to_sequences(list_sentences_test_label)\n",
    "list_tokenized_test_nolabel = tokenizer.texts_to_sequences(list_sentences_test_nolabel)\n",
    "X_tr = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te_label = pad_sequences(list_tokenized_test_label, maxlen=maxlen)\n",
    "X_te_nolabel = pad_sequences(list_tokenized_test_nolabel, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf-8\"))\n",
    "for o in list(embeddings_index.keys()):\n",
    "     if len(embeddings_index[o])!=embed_size:\n",
    "         del embeddings_index[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python365\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)+2)\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[0]=np.zeros((embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inp_l = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp_l)\n",
    "x = Bidirectional(LSTM(150, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(len(list_classes), activation=\"softmax\")(x)\n",
    "model_l = Model(inputs=inp_l, outputs=x)\n",
    "model_l.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_r = Input(shape=(maxlen,))\n",
    "x_r = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp_r)\n",
    "x_r = Bidirectional(LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x_r)\n",
    "x_r = GlobalMaxPool1D()(x_r)\n",
    "x_r = Dense(512, activation=\"relu\")(x_r)\n",
    "x_r = Dropout(0.2)(x_r)\n",
    "x_r = Dense(yr.shape[1], activation=\"sigmoid\")(x_r)\n",
    "model_r = Model(inputs=inp_r, outputs=x_r)\n",
    "model_r.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_m = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp_m)\n",
    "x = Bidirectional(LSTM(150, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(ym.shape[1], activation=\"sigmoid\")(x)\n",
    "model_m = Model(inputs=inp_m, outputs=x)\n",
    "model_m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "inp_c = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp_c)\n",
    "x = Bidirectional(LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(yc.shape[1], activation=\"sigmoid\")(x)\n",
    "model_c = Model(inputs=inp_c, outputs=x)\n",
    "model_c.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=1000\n",
    "tensor_path_l = os.path.join(bpath, \"logs\", \"thinking2_l\")\n",
    "model_path_l = os.path.join(bpath, \"model\", \"thinking2_l\",\"rasa_weights_base.best.hdf5\")\n",
    "checkpoint = ModelCheckpoint(model_path_l, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path_l, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "callbacks_list_l = [checkpoint, early, tensorb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=1000\n",
    "tensor_path_m = os.path.join(bpath, \"logs\", \"thinking2_m\")\n",
    "model_path_m = os.path.join(bpath, \"model\", \"thinking2_m\",\"rasa_weights_base.best.hdf5\")\n",
    "checkpoint = ModelCheckpoint(model_path_m, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path_m, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "callbacks_list_m = [checkpoint, early, tensorb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=1000\n",
    "tensor_path_r = os.path.join(bpath, \"logs\", \"thinking2_r\")\n",
    "model_path_r = os.path.join(bpath, \"model\", \"thinking2_r\",\"rasa_weights_base.best.hdf5\")\n",
    "checkpoint = ModelCheckpoint(model_path_r, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path_r, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "callbacks_list_r = [checkpoint, early, tensorb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=1000\n",
    "tensor_path_c = os.path.join(bpath, \"logs\", \"thinking2_c\")\n",
    "model_path_c = os.path.join(bpath, \"model\", \"thinking2_c\",\"rasa_weights_base.best.hdf5\")\n",
    "checkpoint = ModelCheckpoint(model_path_c, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorb = TensorBoard(log_dir=tensor_path_c, histogram_freq=10, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "callbacks_list_c = [checkpoint, early, tensorb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_c.fit(X_tr, yc, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From d:\\python365\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2241 samples, validate on 250 samples\n",
      "Epoch 1/1000\n",
      "2241/2241 [==============================] - 20s 9ms/step - loss: 0.0699 - acc: 0.9815 - val_loss: 0.0102 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01019, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 2/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0090 - acc: 0.9988 - val_loss: 0.0088 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01019 to 0.00880, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 3/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0086 - acc: 0.9988 - val_loss: 0.0086 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00880 to 0.00861, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 4/1000\n",
      "2241/2241 [==============================] - 19s 8ms/step - loss: 0.0084 - acc: 0.9988 - val_loss: 0.0086 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00861 to 0.00856, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 5/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0084 - acc: 0.9988 - val_loss: 0.0085 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00856 to 0.00848, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 6/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0083 - acc: 0.9988 - val_loss: 0.0085 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00848\n",
      "Epoch 7/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0083 - acc: 0.9988 - val_loss: 0.0084 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00848 to 0.00840, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 8/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0082 - acc: 0.9988 - val_loss: 0.0083 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00840 to 0.00829, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 9/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0081 - acc: 0.9988 - val_loss: 0.0084 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00829\n",
      "Epoch 10/1000\n",
      "2241/2241 [==============================] - 17s 8ms/step - loss: 0.0081 - acc: 0.9988 - val_loss: 0.0083 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00829\n",
      "Epoch 11/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.0083 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00829 to 0.00826, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 12/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0078 - acc: 0.9988 - val_loss: 0.0081 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00826 to 0.00811, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 13/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.0079 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00811 to 0.00793, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 14/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.0078 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00793 to 0.00780, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 15/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.0075 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00780 to 0.00754, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 16/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.0074 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00754 to 0.00744, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 17/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0073 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00744 to 0.00730, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 18/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0072 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00730 to 0.00722, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 19/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0072 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00722 to 0.00718, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 20/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0070 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00718 to 0.00704, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 21/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0070 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00704 to 0.00698, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 22/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0068 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00698 to 0.00683, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 23/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0068 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00683\n",
      "Epoch 24/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0067 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00683 to 0.00666, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 25/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0066 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00666 to 0.00655, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 26/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0065 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00655 to 0.00649, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 27/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0064 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00649 to 0.00635, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 28/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0063 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00635 to 0.00634, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 29/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0063 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00634 to 0.00627, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 30/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0063 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00627 to 0.00626, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 31/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0062 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00626 to 0.00616, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 32/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0061 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00616 to 0.00609, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 33/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0061 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00609 to 0.00608, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 34/1000\n",
      "2241/2241 [==============================] - 19s 8ms/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0061 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00608\n",
      "Epoch 35/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0060 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00608 to 0.00604, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 36/1000\n",
      "2241/2241 [==============================] - 19s 8ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0060 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00604\n",
      "Epoch 37/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0060 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00604 to 0.00599, saving model to ..\\data\\model\\thinking2_m\\rasa_weights_base.best.hdf5\n",
      "Epoch 38/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0060 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00599\n",
      "Epoch 39/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0060 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00599\n",
      "Epoch 40/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0061 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00599\n",
      "Epoch 41/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0061 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00599\n",
      "Epoch 42/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0062 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00599\n",
      "Epoch 43/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0062 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00599\n",
      "Epoch 44/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0061 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00599\n",
      "Epoch 45/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0062 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00599\n",
      "Epoch 46/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0063 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00599\n",
      "Epoch 47/1000\n",
      "2241/2241 [==============================] - 18s 8ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0062 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29cbe004828>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.fit(X_tr, ym, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_r.fit(X_tr, yr, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_l.fit(X_tr, yl, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [],
   "source": [
    "# y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "# sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "# sample_submission[list_classes] = y_test\n",
    "# sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_l.load_weights(model_path_l)\n",
    "# y_test_l = model_l.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_r.load_weights(model_path_r)\n",
    "# y_test_r = model_r.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.load_weights(model_path_m)\n",
    "y_test_m_label = model_m.predict(X_te_label)\n",
    "y_test_m_nolabel = model_m.predict(X_te_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_c.load_weights(model_path_c)\n",
    "# y_test_c = model_c.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'level', 'mainReviewPoints', 'reviewPoints', 'text'], dtype='object')\n",
      "Index(['id', 'level', 'mainReviewPoints', 'reviewPoints', 'text'], dtype='object')\n",
      "(277, 861)\n",
      "(1029, 861)\n",
      "finish output csv.\n"
     ]
    }
   ],
   "source": [
    "sample_submission_label = pd.read_csv(TMP_LABEL_FILE, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "sample_submission_nolabel = pd.read_csv(TMP_NOLABEL_FILE, header=0, encoding=\"utf8\", dtype=str,sep=',')\n",
    "print(sample_submission_label.columns)\n",
    "print(sample_submission_nolabel.columns)\n",
    "sample_submission_label=sample_submission_label[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\"]]\n",
    "sample_submission_nolabel=sample_submission_nolabel[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\"]]\n",
    "# sample_submission=sample_submission[[\"Description\", \"Level\"]]\n",
    "# 结果转化输出\n",
    "\n",
    "reshape = y_test_m_label.shape\n",
    "print(reshape)\n",
    "flist_m = []\n",
    "flist_r = []\n",
    "for i1 in range(reshape[0]):\n",
    "    strlist_m = []\n",
    "    strlist_r = []\n",
    "    for i2 in range(reshape[1]):\n",
    "        if y_test_m_label[i1,i2] > 0.5:\n",
    "            if i2 >= label_lenth:\n",
    "                strlist_r.append(map_points[label_list[i2-label_lenth]])\n",
    "            else:\n",
    "                strlist_m.append(map_points[label_list[i2]])\n",
    "    flist_m.append(\",\".join(strlist_m))\n",
    "    flist_r.append(\",\".join(strlist_r))\n",
    "np_flist_m = np.array(flist_m)\n",
    "np_flist_r = np.array(flist_r)\n",
    "sample_submission_label[\"mainReviewPoints_new\"] = np_flist_m\n",
    "sample_submission_label=sample_submission_label[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\",\"mainReviewPoints_new\"]]\n",
    "sample_submission_label.to_csv(TMP_LABEL_FILE, index=False)\n",
    "\n",
    "reshape = y_test_m_nolabel.shape\n",
    "print(reshape)\n",
    "flist_m = []\n",
    "flist_r = []\n",
    "for i1 in range(reshape[0]):\n",
    "    strlist_m = []\n",
    "    strlist_r = []\n",
    "    for i2 in range(reshape[1]):\n",
    "        if y_test_m_nolabel[i1,i2] > 0.5:\n",
    "            if i2 >= label_lenth:\n",
    "                strlist_r.append(map_points[label_list[i2-label_lenth]])\n",
    "            else:\n",
    "                strlist_m.append(map_points[label_list[i2]])\n",
    "    flist_m.append(\",\".join(strlist_m))\n",
    "    flist_r.append(\",\".join(strlist_r))\n",
    "np_flist_m = np.array(flist_m)\n",
    "np_flist_r = np.array(flist_r)\n",
    "sample_submission_nolabel[\"mainReviewPoints_new\"] = np_flist_m\n",
    "# sample_submission[\"reviewPoints_new\"] = np_flist_r\n",
    "\n",
    "# for indexs in sample_submission.index:  \n",
    "#     for  i2 in list_classes:  \n",
    "#         if(sample_submission.loc[indexs,i2] ==sample_submission.loc[indexs,\"max\"]):\n",
    "#             sample_submission.loc[indexs,\"predict\"]=i2\n",
    "# for i1 in list_classes:\n",
    "#     sample_submission.rename(columns={i1: \"pred_\" + i1}, inplace=True)\n",
    "sample_submission_nolabel=sample_submission_nolabel[[\"id\", \"text\", \"mainReviewPoints\", \"reviewPoints\", \"level\",\"mainReviewPoints_new\"]]\n",
    "sample_submission_nolabel.to_csv(TMP_NOLABEL_FILE, index=False)\n",
    "# print(sample_submission[\"mainReviewPoints\"])\n",
    "# print(sample_submission[\"mainReviewPoints\"][0])\n",
    "# print(type(sample_submission[\"mainReviewPoints\"][0]))\n",
    "# print(sample_submission[sample_submission[\"mainReviewPoints\"]!=\"\"])\n",
    "print(\"finish output csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File ..\\data\\thinking2\\models\\tmp_model\\tmp_model.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8814b1b5ccb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"thinking2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mheadname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}.meta'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}.meta'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python365\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[0;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[1;32m-> 1674\u001b[1;33m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[0;32m   1675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python365\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[0;32m   1684\u001b[0m                        \"execution is enabled.\")\n\u001b[0;32m   1685\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1686\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1687\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python365\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File %s does not exist.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m   \u001b[1;31m# First try to read it as a binary file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File ..\\data\\thinking2\\models\\tmp_model\\tmp_model.meta does not exist."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "\n",
    "bpath = os.path.join(\"..\", \"data\")\n",
    "model_name = \"tmp_model\"\n",
    "if model_name is None:\n",
    "    model_name = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "model_dir = os.path.join(bpath, \"thinking2\", \"models\", model_name)\n",
    "headname = os.path.join(model_dir,model_name)\n",
    "saver = tf.train.import_meta_graph('{}.meta'.format(headname))\n",
    "print('{}.meta'.format(headname))\n",
    "print(saver)\n",
    "checkpoint = tf.train.Checkpoint(myAwesomeModel=model_to_be_restored, myAwesomeOptimizer=optimizer)\n",
    "saver.restore(tf.get_default_session(), 'save/filename.ckpt-16000')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
